{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import scipy\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_emot():\n",
    "    e = [\"(o_o)\",\":-)\",\":P\",\":D\",\"x)\",\"ᓚᘏᗢ\",\"╯°□°）╯︵ ┻━┻\",\":)\",\n",
    "         \"*<:-)\",\"^_^\",\"(⌐■_■)\",\"¯\\_(ツ)_/¯\", \"(T_T)\",\":o\",\"OwO\",\n",
    "        \"( ͡❛ ͜ʖ ͡❛)\",\"(̶◉͛‿◉̶)\",\"( ≖.≖)\",\"(ㆆ_ㆆ)\",\"ʕ•́ᴥ•̀ʔっ\",\"( ◡́.◡̀)\",\"(^◡^ )\"]\n",
    "    return random.choice(e)\n",
    "\n",
    "def load_files():\n",
    "    text_pairs = [] #Would be nice to have as np.array\n",
    "    labels = []\n",
    "    fandom = []\n",
    "    \n",
    "    pair_id = []\n",
    "    true_id = []\n",
    "    \n",
    "    #Load truth JSON\n",
    "    for line in open('data/modified/train_truth.jsonl'):\n",
    "        d = json.loads(line.strip())\n",
    "        labels.append(int(d['same']))\n",
    "        true_id.append(d['id'])\n",
    "\n",
    "    #Load actual fanfic.\n",
    "    print(\"loading fanfic...\",rand_emot())\n",
    "    for line in tqdm(open('data/modified/train_pair.jsonl')):\n",
    "        d = json.loads(line.strip())\n",
    "        text_pairs.append(d['pair'])\n",
    "        fandom.append(d['fandoms'])\n",
    "        pair_id.append(d['id'])\n",
    "\n",
    "    print(\"done loading\",rand_emot())\n",
    "    \n",
    "    return text_pairs, labels, fandom, pair_id, true_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "509it [00:00, 2503.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading fanfic... (o_o)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15780it [00:06, 2532.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading :o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_pairs, labels, fandom, pair_id, true_id = load_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {\n",
    "    'character_bigram':None, #Character bigram\n",
    "    'skip_bigram': None, #Word skipgram\n",
    "    'pos_skipgram': None, #POS skipgram\n",
    "    'pos_bigram' : None, #POS bigram\n",
    "    'character_bigram_cossim':None, #Character bigram\n",
    "    'skip_bigram_cossim': None, #Word skipgram\n",
    "    'pos_skipgram_cossim': None, #POS skipgram\n",
    "    'pos_bigram_cossim' : None, #POS bigram\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dumb_text = 11550\n",
    "\n",
    "text_pairs.pop(dumb_text)\n",
    "labels.pop(dumb_text)\n",
    "#egegege #FIND OUT WHICH INDEX IS WEIRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(text_pairs):\n",
    "    '''input all text pairs to create a corpus'''\n",
    "    corpus = [x[i] for x in text_pairs for i in range(len(x))]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = create_corpus(text_pairs)\n",
    "labels = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_me_pca(vector, labels, is_pairs=False, title=\"\"):\n",
    "    '''Plot PCA for the two classes. Input is one long vector/list, it creates pairs itself.\n",
    "    If you already created pairs, use is_pairs=True\n",
    "    Example: if vector is distance between pairs, do not set is_pairs to True. \n",
    "    (as it is one vector describing both documents.)\n",
    "    \n",
    "    labels is simply a vector with the labels, which will be used to colour the scatter plot.'''\n",
    "    \n",
    "    #Convert labels to np.array (might be a list.)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    #Join pairs into one, long vector if necessary. \n",
    "    if not is_pairs:\n",
    "        if scipy.sparse.issparse(vector):\n",
    "            vector = [np.hstack([vector[x],vector[x+1]]) for x in range(0,vector.shape[0],2)]  \n",
    "        else:\n",
    "            vector = [np.hstack([vector[x],vector[x+1]]) for x in range(0,len(vector),2)]  \n",
    "    \n",
    "    #Get that PCA - Use SVD if vector is sparse.\n",
    "    if scipy.sparse.issparse(vector):\n",
    "        \n",
    "        pca = TruncatedSVD(n_components=2)\n",
    "    else:\n",
    "        pca = PCA(n_components=2)\n",
    "    pcs = pca.fit_transform(vector)\n",
    "    \n",
    "    #Printing pcs shape - remember they might be halved, due to pairing. \n",
    "    print(pcs.shape)\n",
    "    \n",
    "    #Group PC's into two, according to label indices. \n",
    "    group1 = pcs[labels==0]\n",
    "    group2 = pcs[labels==1]\n",
    "    \n",
    "    #Plot that shit!\n",
    "    plt.scatter(group1[:,0], group1[:,1], s=5)\n",
    "    plt.scatter(group2[:,0], group2[:,1], s=5)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Word Feature Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolate function words\n",
    "\n",
    "with open('data/function_words_clean.txt', \"r\") as fw:\n",
    "    func_words = fw.read().split()\n",
    "\n",
    "def isolate_fw(corpus, f_words): #data must be json file - input must be path to file \n",
    "    fw_in_data = []\n",
    "\n",
    "    for text in tqdm(corpus):\n",
    "        function_words = []\n",
    "        words = text.split() #split fanfic into words in list\n",
    "        for word in words: \n",
    "            if word in f_words: #if the word is a function word\n",
    "                function_words.append(word)\n",
    "                \n",
    "        stringed_function_words = \" \".join(function_words)\n",
    "        \n",
    "        #append all function words as one long string in a list\n",
    "        fw_in_data.append(stringed_function_words) #fw_in_data is a list with strings\n",
    "        #each list contains a string of all function words for each pair\n",
    "        #should it be a string for each pair?\n",
    "        \n",
    "    return fw_in_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function_words = isolate_fw(corpus, func_words)\n",
    "#fw_tokens = [nltk.word_tokenize(x) for x in function_words]\n",
    "#unigram_freqdists = [nltk.FreqDist(x) for x in fw_tokens]\n",
    "#bigrams = [nltk.bigrams(x) for x in fw_tokens]\n",
    "#bigram_freqdists = [nltk.FreqDist(x) for x in bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mat_from_fdists(fdists):\n",
    "    '''This function compiles a list of frequency distributions and create a matrix with feature vectors.\n",
    "    Each row is a feature vector. Should would with all typed of dictionaries, actually.'''\n",
    "    all_keys = list(set([item for sublist in fdists for item in sublist]))\n",
    "    matrix = np.zeros((len(fdists),len(all_keys)))\n",
    "    \n",
    "    for i, fd in enumerate(tqdm(fdists)):\n",
    "        for j, key in enumerate(all_keys):\n",
    "            matrix[i,j] = fd[key]\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v1 = normalize(get_mat_from_fdists(unigram_freqdists))\n",
    "#v1 = normalize(get_mat_from_fdists(bigram_freqdists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_character_ngram(n, corpus):\n",
    "    #Will return matrix where each row is a pair of texts\n",
    "    vectorizer = TfidfVectorizer(max_features=3000, analyzer='char',ngram_range=(n, n))\n",
    "    char_bigram = vectorizer.fit_transform(corpus).toarray()\n",
    "    return char_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_bigrams = create_character_ngram(2, corpus)\n",
    "char_bigrams_pair = np.array([np.concatenate((char_bigrams[x], char_bigrams[x+1])) for x in range(0,char_bigrams.shape[0],2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save features function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_features(feature_dict, filename='features'):\n",
    "    '''Save the updated feature dictionary. Takes dictionary as input and saves as binary file\n",
    "    \n",
    "    example: \n",
    "    >>> my_featues = {'freqdist': [1,6,3,5]}\n",
    "    >>> save_features(my_features)'''\n",
    "    \n",
    "    with open('data/{}.dat'.format(filename), 'wb') as file:\n",
    "        pickle.dump(feature_dict, file)\n",
    "    print(\"Features saved! :-)\")\n",
    "\n",
    "def load_features(filename='features'):\n",
    "    '''Load feature dictionary. Returns the saved feature as a dictionary.\n",
    "    \n",
    "    example: \n",
    "    >>> my_features = load_features()'''\n",
    "    \n",
    "    with open('data/{}.dat'.format(filename), 'rb') as file:\n",
    "        feats = pickle.load(file)\n",
    "    print(\"Features available:\")\n",
    "    for i in feats.keys():\n",
    "        print(i)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stolen from https://stackoverflow.com/questions/31847682/how-to-compute-skipgrams-in-python\n",
    "from itertools import chain, combinations\n",
    "import copy\n",
    "from nltk.util import ngrams\n",
    "\n",
    "corpus_tokens = [nltk.word_tokenize(x) for x in corpus]\n",
    "\n",
    "def pad_sequence(sequence, n, pad_left=False, pad_right=False, pad_symbol=None):\n",
    "    if pad_left:\n",
    "        sequence = chain((pad_symbol,) * (n-1), sequence)\n",
    "    if pad_right:\n",
    "        sequence = chain(sequence, (pad_symbol,) * (n-1))\n",
    "    return sequence\n",
    "\n",
    "def skipgrams(sequence, n, k, pad_left=False, pad_right=False, pad_symbol=None):\n",
    "    sequence_length = len(sequence)\n",
    "    sequence = iter(sequence)\n",
    "    sequence = pad_sequence(sequence, n, pad_left, pad_right, pad_symbol)\n",
    "\n",
    "    if sequence_length + pad_left + pad_right < k:\n",
    "        raise Exception(\"The length of sentence + padding(s) < skip\")\n",
    "\n",
    "    if n < k:\n",
    "        raise Exception(\"Degree of Ngrams (n) needs to be bigger than skip (k)\")    \n",
    "\n",
    "    history = []\n",
    "    nk = n+k\n",
    "\n",
    "    # Return point for recursion.\n",
    "    if nk < 1: \n",
    "        return\n",
    "    # If n+k longer than sequence, reduce k by 1 and recur\n",
    "    elif nk > sequence_length: \n",
    "        for ng in skipgrams(list(sequence), n, k-1):\n",
    "            yield ng\n",
    "\n",
    "    while nk > 1: # Collects the first instance of n+k length history\n",
    "        history.append(next(sequence))\n",
    "        nk -= 1\n",
    "\n",
    "    # Iterative drop first item in history and picks up the next\n",
    "    # while yielding skipgrams for each iteration.\n",
    "    for item in sequence:\n",
    "        history.append(item)\n",
    "        current_token = history.pop(0)      \n",
    "        # Iterates through the rest of the history and \n",
    "        # pick out all combinations the n-1grams\n",
    "        for idx in list(combinations(range(len(history)), n-1)):\n",
    "            ng = [current_token]\n",
    "            for _id in idx:\n",
    "                ng.append(history[_id])\n",
    "            yield tuple(ng)\n",
    "\n",
    "    # Recursively yield the skigrams for the rest of seqeunce where\n",
    "    # len(sequence) < n+k\n",
    "    for ng in list(skipgrams(history, n, k-1)):\n",
    "        yield ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 31558/31558 [11:06<00:00, 47.35it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 15779/15779 [00:01<00:00, 11526.39it/s]\n"
     ]
    }
   ],
   "source": [
    "def create_skip_gram(input_text, n=2, k=2):\n",
    "    #Creates count vector of skip-gram of text. (Output is paired)\n",
    "    n,k=2,2\n",
    "    vectorizer = TfidfVectorizer(max_features=4000, ngram_range=(n, n))\n",
    "    skipgram = []\n",
    "    for i in tqdm(range(len(input_text))):\n",
    "        #Join skipgrams to one single text for tfidf - will use ngrams anyway.\n",
    "        skipgram.append(' '.join([' '.join(x) for x in list(skipgrams(input_text[i], n=n, k=k))]))\n",
    "\n",
    "    fd_mat = vectorizer.fit_transform(skipgram).toarray()\n",
    "    return fd_mat\n",
    "\n",
    "skipgram_bigram = create_skip_gram(corpus_tokens)\n",
    "skipgram_bigram_pair = np.array([np.concatenate((skipgram_bigram[x], skipgram_bigram[x+1])) for x in tqdm(range(0,len(skipgram_bigram),2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 31558/31558 [48:23<00:00, 10.87it/s]\n"
     ]
    }
   ],
   "source": [
    "pos_tags = [nltk.pos_tag(x) for x in corpus_tokens]\n",
    "\n",
    "single_pos_tags = []\n",
    "for text in pos_tags:\n",
    "    text_tags = []\n",
    "    for tags in text:\n",
    "        text_tags.append(tags[1])\n",
    "    single_pos_tags.append(text_tags)\n",
    "\n",
    "POS_skipgram = create_skip_gram(single_pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 15779/15779 [00:00<00:00, 19162.31it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 15779/15779 [00:01<00:00, 15502.32it/s]\n"
     ]
    }
   ],
   "source": [
    "POS_skipgram_pair = np.array([np.concatenate((POS_skipgram[x], POS_skipgram[x+1])) for x in tqdm(range(0,len(POS_skipgram),2))])\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=3000, ngram_range=(2, 2))\n",
    "POS_bigram = vectorizer.fit_transform(corpus).toarray()\n",
    "POS_bigram_pair = np.array([np.concatenate((POS_bigram[x], POS_bigram[x+1])) for x in tqdm(range(0,len(POS_bigram),2))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_bigrams_cosine = np.array([cosine_sim(char_bigrams[x], char_bigrams[x+1]) for x in range(0, char_bigrams.shape[0],2)])\n",
    "skip_bigrams_cosine = np.array([cosine_sim(skipgram_bigram[x], skipgram_bigram[x+1]) for x in range(0, skipgram_bigram.shape[0],2)])\n",
    "pos_bigrams_cosine = np.array([cosine_sim(POS_bigram[x], POS_bigram[x+1]) for x in range(0, POS_bigram.shape[0],2)])\n",
    "pos_skipgram_cosine = np.array([cosine_sim(POS_skipgram[x], POS_skipgram[x+1]) for x in range(0, POS_skipgram.shape[0],2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['character_bigram'] = char_bigrams_pair\n",
    "features['skip_bigram'] = skipgram_bigram_pair\n",
    "features['pos_skipgram'] = POS_skipgram_pair\n",
    "features['pos_bigram'] = POS_bigram_pair\n",
    "features['character_bigram_cossim'] = char_bigrams_cosine\n",
    "features['skip_bigram_cossim'] = skip_bigrams_cosine\n",
    "features['pos_skipgram_cossim'] = pos_skipgram_cosine\n",
    "features['pos_bigram_cossim'] = pos_bigrams_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved! :-)\n"
     ]
    }
   ],
   "source": [
    "save_features(features, filename=\"big_feats_0706\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character_bigram <class 'numpy.ndarray'>\n",
      "skip_bigram <class 'numpy.ndarray'>\n",
      "pos_skipgram <class 'numpy.ndarray'>\n",
      "pos_bigram <class 'numpy.ndarray'>\n",
      "character_bigram_cossim <class 'numpy.ndarray'>\n",
      "skip_bigram_cossim <class 'numpy.ndarray'>\n",
      "pos_skipgram_cossim <class 'numpy.ndarray'>\n",
      "pos_bigram_cossim <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "for i in features:\n",
    "    print(i, type(features[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = load_features(filename='feats_0706')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yasmin Features joining\n",
    "fw distribution, profanity distribution, character bigram dist., unique words, punctuation proportion, word len, sent len, yules, lix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_matrix = np.load(\"feature_matrix.dat\", allow_pickle=True)\n",
    "feat_matrix = np.delete(feat_matrix, dumb_text, 0)\n",
    "\n",
    "features['function_word_dist'] = feat_matrix[:,0]\n",
    "features['profanity_dist'] = feat_matrix[:,1]\n",
    "features['unique_words'] = feat_matrix[:,3]\n",
    "features['punctuation_proportion'] = feat_matrix[:,4]\n",
    "features['avg_word_length'] = feat_matrix[:,5]\n",
    "features['avg_sent_length'] = feat_matrix[:,6]\n",
    "features['yules'] = feat_matrix[:,7]\n",
    "features['lix']= feat_matrix[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved! :-)\n"
     ]
    }
   ],
   "source": [
    "save_features(features, filename='big_features_yasmin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15779,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['lix'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
