{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_emot():\n",
    "    e = [\"(o_o)\",\":-)\",\":P\",\":D\",\"x)\",\"ᓚᘏᗢ\",\"╯°□°）╯︵ ┻━┻\",\":)\",\n",
    "         \"*<:-)\",\"^_^\",\"(⌐■_■)\",\"¯\\_(ツ)_/¯\", \"(T_T)\",\":o\",\"OwO\",\n",
    "        \"( ͡❛ ͜ʖ ͡❛)\",\"(̶◉͛‿◉̶)\",\"( ≖.≖)\",\"(ㆆ_ㆆ)\",\"ʕ•́ᴥ•̀ʔっ\",\"( ◡́.◡̀)\",\"(^◡^ )\"]\n",
    "    return random.choice(e)\n",
    "\n",
    "def load_files():\n",
    "    text_pairs = [] #Would be nice to have as np.array\n",
    "    labels = []\n",
    "    fandom = []\n",
    "    \n",
    "    pair_id = []\n",
    "    true_id = []\n",
    "    \n",
    "    #Load truth JSON\n",
    "    for line in open('data/modified/train_truth.jsonl'):\n",
    "        d = json.loads(line.strip())\n",
    "        labels.append(int(d['same']))\n",
    "        true_id.append(d['id'])\n",
    "\n",
    "    #Load actual fanfic.\n",
    "    print(\"loading fanfic...\",rand_emot())\n",
    "    for line in tqdm(open('data/modified/train_pair.jsonl')):\n",
    "        d = json.loads(line.strip())\n",
    "        text_pairs.append(d['pair'])\n",
    "        fandom.append(d['fandoms'])\n",
    "        pair_id.append(d['id'])\n",
    "\n",
    "    print(\"done loading\",rand_emot())\n",
    "    \n",
    "    return text_pairs, labels, fandom, pair_id, true_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "178it [00:00, 1763.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading fanfic... *<:-)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1578it [00:00, 1786.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading ¯\\_(ツ)_/¯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_pairs, labels, fandom, pair_id, true_id = load_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word frequency and word frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_distribution(text_pair): #expect untokenized input\n",
    "    \n",
    "    pair = []\n",
    "    \n",
    "    for text in text_pair: \n",
    "        tokens = nltk.word_tokenize(text) #tokenize\n",
    "        \n",
    "        freq_dist = nltk.FreqDist(tokens) #compute frequency distribution\n",
    "        pair.append(freq_dist)\n",
    "        \n",
    "    return pair #return frequency distribution of each fanfic in the input pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq(text_pair): #expects tokenized pairs\n",
    "    fdist0 = nltk.FreqDist(text_pair[0])\n",
    "    fdist1 = nltk.FreqDist(text_pair[1])\n",
    "    \n",
    "    return [fdist0, fdist1]\n",
    "\n",
    "def word_freq_single(text):\n",
    "    fdist = nltk.FreqDist(text)\n",
    "    return fdist\n",
    "\n",
    "def tokenize(text_pair):\n",
    "    return [nltk.word_tokenize(text_pair[0]),nltk.word_tokenize(text_pair[1])]\n",
    "\n",
    "def vector_freq_dist(freq_dists): #I don't think this works...\n",
    "    return [list(freq_dists[0].values()), list(freq_dists[1].values())]\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)+0.0000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(text_pairs):\n",
    "    '''input all text pairs to create a corpus'''\n",
    "    corpus = [x[i] for x in text_pairs for i in range(len(x))]\n",
    "    return corpus\n",
    "\n",
    "def fit_tfidf(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    print(\"training vectorizer...\",rand_emot())\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    print(\"vectorizer fit!\", rand_emot())\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(X[0].T.todense(), index=vectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "    df = df.sort_values('TF-IDF', ascending=False)\n",
    "    \n",
    "    return X, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = create_corpus(text_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training vectorizer... ( ≖.≖)\n",
      "vectorizer fit! ╯°□°）╯︵ ┻━┻\n"
     ]
    }
   ],
   "source": [
    "#tf-idf on the raw text. Likely not useful, as you can see, it is sesnitive to the fandom.\n",
    "raw_tfidf, tfidf_df = fit_tfidf(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_tfidf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_words(text_pairs):\n",
    "    \n",
    "    richness_all = []\n",
    "    \n",
    "    for pair in tqdm(text_pairs):\n",
    "        richness_pair = []\n",
    "        \n",
    "        for text in pair: \n",
    "            tokens = nltk.word_tokenize(text.lower())\n",
    "            unique_tokens = list(set(tokens))\n",
    "            \n",
    "            richness = len(unique_tokens) / len(tokens)\n",
    "            richness_pair.append(richness)\n",
    "        richness_all.append(richness_pair)\n",
    "    \n",
    "    feature = []\n",
    "    \n",
    "    for i in range(len(text_pairs)):\n",
    "        rich_feat = richness_all[i][0] - richness_all[i][1]\n",
    "        feature.append(np.abs(rich_feat))\n",
    "    \n",
    "    feature = np.vstack(feature)\n",
    "\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1578/1578 [03:39<00:00,  7.19it/s]\n"
     ]
    }
   ],
   "source": [
    "richness_feature = unique_words(text_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempting to perform tf-idf on only symbols.\n",
    "def isolate_symbols(corpus):\n",
    "    #Add \\d to omit digits too.\n",
    "    sym_corpus = []\n",
    "    for text in corpus:\n",
    "        sym_corpus.append(' '.join(re.findall(\"[^a-zA-Z\\s]+\", text)))\n",
    "    return sym_corpus\n",
    "\n",
    "symbols = isolate_symbols(corpus)\n",
    "\n",
    "#Okay, tf-idf doesn't work with symbols. I'll convert them to made-up words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training vectorizer... x)\n",
      "vectorizer fit! ʕ•́ᴥ•̀ʔっ\n"
     ]
    }
   ],
   "source": [
    "punct_matrix, punct_DF = fit_tfidf(symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. punctuation divided by total no. tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make this a little better - fewer lines\n",
    "\n",
    "def punctuation_proportion(text_pairs, corpus, punctuation_corpus):\n",
    "    feature = []\n",
    "    \n",
    "    punc_prop_all = [] #punctuation proportion for all pairs \n",
    "    \n",
    "    for i in range(len(text_pairs)): \n",
    "        punc_prop_pair = [] #punctuation proportion for each pair\n",
    "        \n",
    "        punc_prop1 = len(symbols[2*i]) / len(corpus[2*i])\n",
    "        punc_prop2 = len(symbols[2*i+1]) / len(corpus[2*i+1])\n",
    "        punc_prop_pair.append(punc_prop1)\n",
    "        punc_prop_pair.append(punc_prop2)\n",
    "        punc_prop_all.append(punc_prop_pair)\n",
    "    \n",
    "    for i in range(len(text_pairs)):\n",
    "        feature_pair = punc_prop_all[i][0] - punc_prop_all[i][1]\n",
    "        feature.append(feature_pair)\n",
    "        \n",
    "    feature = np.vstack(np.abs(feature))\n",
    "        \n",
    "    return feature\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_proportion_feature = punctuation_proportion(text_pairs, corpus, symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS-tagging and Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## POS Tagging and ngrams\n",
    "tokens = nltk.word_tokenize(corpus[0])\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "pos_bigrams = nltk.bigrams(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIX calculation - LIX = readability index, a measure for the readability of a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lix(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    splt = text.split()\n",
    "    o = len(splt)+1\n",
    "    p = len([x for x in tokens if x=='.'])+1\n",
    "    l = len([x for x in tokens if len(x)>6])+1\n",
    "    \n",
    "    return (o/p)+((l*100)/o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text in corpus[:10]:\n",
    "#     print(compute_lix(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1578/1578 [03:11<00:00,  8.24it/s]\n"
     ]
    }
   ],
   "source": [
    "lix_feature = []\n",
    "\n",
    "for i in tqdm(range(len(text_pairs))):\n",
    "    lix = compute_lix(corpus[2*1]) - compute_lix(corpus[2*i+1])\n",
    "    lix_feature.append(np.abs(lix))\n",
    "    \n",
    "lix_feature = np.vstack(lix_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lix_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence and word length - compute sentence and word length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols(text):\n",
    "    sentences = re.split('[\\.+|!|?]', text)\n",
    "    sentences = [re.sub(r\"[^\\w]+\", ' ', x) for x in sentences if len(x.strip()) != 0]\n",
    "    return ' '.join(sentences)\n",
    "\n",
    "def get_sent_word_length(text):\n",
    "    #Function, which removes symbols and count words in sentence\n",
    "    #Output: length of each sentence & length of each word\n",
    "    sentences = re.split('[\\.+|!|?]', text)\n",
    "    sentences = [re.sub(r\"[^\\w]+\", ' ', x) for x in sentences if len(x.strip()) != 0]\n",
    "    word_sentences = [nltk.word_tokenize(x) for x in sentences]\n",
    "    sentence_lengths = np.array([len(x) for x in word_sentences])\n",
    "    word_lengths = np.array([len(s) for x in word_sentences for s in x])\n",
    "    return sentence_lengths, word_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def avg_sent_word_length(text_pairs):\n",
    "    \n",
    "    avg_sent_len = []\n",
    "    avg_word_len = []\n",
    "\n",
    "    for i in tqdm(range(len(text_pairs))):\n",
    "        sent_length1, word_lengths1 = get_sent_word_length(corpus[i*2])\n",
    "        sent_length2, word_lengths2 = get_sent_word_length(corpus[i*2+1])\n",
    "\n",
    "        avg_sent = np.average(sent_length1) - np.average(sent_length2)\n",
    "        avg_word = np.average(word_lengths1) - np.average(word_lengths2)\n",
    "\n",
    "        avg_sent_len.append(np.abs(avg_sent))\n",
    "        avg_word_len.append(np.abs(avg_word))\n",
    "        \n",
    "    avg_sent_len = np.vstack(avg_sent_len)\n",
    "    avg_word_len = np.vstack(avg_word_len)\n",
    "        \n",
    "        \n",
    "    return avg_word_len, avg_sent_len #returns feature vector of average word and sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1578/1578 [03:34<00:00,  7.36it/s]\n"
     ]
    }
   ],
   "source": [
    "word_len_feature, sent_len_feature = avg_sent_word_length(text_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolating function words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/function_words_clean.txt', \"r\") as fw:\n",
    "    func_words = fw.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function isolates function words and return the feature (cosine similarity between texts in pair)\n",
    "# along with the isolated functions words (for the function word proportion feature\n",
    "\n",
    "def isolate_function_words(text_pairs, f_words): #data must be the text_pairs from load_files()\n",
    "    fw_in_data = []\n",
    "    \n",
    "    fw_text_pairs = []\n",
    "    for pair in tqdm(text_pairs):\n",
    "        fw_text_pairs = []\n",
    "        for text in pair: \n",
    "            function_words = []\n",
    "            \n",
    "            words = text.split() #split fanfic into words in list\n",
    "            \n",
    "            for word in words: \n",
    "                if word in f_words: #if the word is a function word\n",
    "                    function_words.append(word)\n",
    "                \n",
    "            stringed_function_words = \" \".join(function_words) #for each fanfic in a pair, makes FW a long string. \n",
    "            fw_text_pairs.append(stringed_function_words) \n",
    "            \n",
    "        #append text pairs with only their function words\n",
    "        fw_in_data.append(fw_text_pairs) \n",
    "        \n",
    "    fw_corpus = create_corpus(fw_in_data)  \n",
    "    FW_matrix, fw_dateframe = fit_tfidf(fw_corpus) #vectorize\n",
    "    fw_matrix = FW_matrix.toarray()\n",
    "    \n",
    "    feature = []\n",
    "\n",
    "    for i in range(len(text_pairs)):\n",
    "        cos_sim = cosine_sim(fw_matrix[2*i], fw_matrix[2*i+1]) #compute similarity \n",
    "        feature.append(cos_sim)\n",
    "        \n",
    "    feature = np.vstack(feature) #final feature stack\n",
    "    \n",
    "        \n",
    "    return feature, fw_in_data #returns feature vector and each text pair with only their function words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1578/1578 [00:56<00:00, 27.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training vectorizer... (^◡^ )\n",
      "vectorizer fit! OwO\n"
     ]
    }
   ],
   "source": [
    "function_words_feature, fw_in_data = isolate_function_words(text_pairs, func_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function words divided by total no. tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_words_proportion(text_pair_original, text_pair_fw): \n",
    "    fw_by_total = []\n",
    "    \n",
    "    for i, pair in enumerate(text_pair_original): \n",
    "        fw_by_total_pair = []\n",
    "        for ind, text in enumerate(pair): \n",
    "            fw_length = len(text_pair_fw[i][ind]) #length of text counting only function words\n",
    "            orig_length = len(text_pair_original[i][ind]) #length of text counting all tokens\n",
    "            proportion = fw_length/orig_length #divide occurance of function words by all tokens\n",
    "            fw_by_total_pair.append(proportion)\n",
    "        fw_by_total.append(fw_by_total_pair)\n",
    "        \n",
    "        \n",
    "    feature = []\n",
    "\n",
    "    for pair in fw_by_total: \n",
    "        feat = pair[0] - pair[1]\n",
    "        feature.append(np.abs(feat))\n",
    "\n",
    "    feature = np.vstack(feature)\n",
    "    \n",
    "    \n",
    "    return feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw_proportion_feature = function_words_proportion(text_pairs, fw_in_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolating profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'data/modified/train_pair.jsonl'\n",
    "with open('data/profanity_words_clean.txt', \"r\") as pr:\n",
    "    prof_words = pr.read().split()\n",
    "del prof_words[:4]\n",
    "del prof_words[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'data/modified/train_pair.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_profanity(data, prof_words):\n",
    "    profanity = []\n",
    "    \n",
    "    for pair in tqdm(data):\n",
    "        profanity_pairs = []\n",
    "        for text in pair:\n",
    "            resultwords = []\n",
    "\n",
    "            #d = json.loads(line.strip())\n",
    "            #text = d.get(\"pair\") \n",
    "            words = text.split() \n",
    "\n",
    "            resultwords  = [word for word in words if word.lower() in prof_words]\n",
    "\n",
    "            result = \" \".join(resultwords)\n",
    "            profanity_pairs.append(result)\n",
    "        \n",
    "        profanity.append(profanity_pairs) \n",
    "        \n",
    "    profanity_corpus = create_corpus(profanity)\n",
    "    profanity_matrix, profanity_dataframe = fit_tfidf(profanity_corpus)\n",
    "    profanity_matrix = profanity_matrix.toarray()\n",
    "    \n",
    "    feature = []\n",
    "\n",
    "    for i in range(len(text_pairs)):\n",
    "        cos_sim = cosine_sim(profanity_matrix[2*i], profanity_matrix[2*i+1])\n",
    "        feature.append(cos_sim)\n",
    "        \n",
    "    feature = np.vstack(feature)\n",
    "\n",
    "        \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1578/1578 [02:11<00:00, 11.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training vectorizer... :-)\n",
      "vectorizer fit! ( ͡❛ ͜ʖ ͡❛)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "profanity_feature = isolate_profanity(text_pairs, prof_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yule's K computations - different implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Our own implementation - delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_no_symbols(text):\n",
    "    return nltk.word_tokenize(re.sub(r'[^\\w]', ' ', text))\n",
    "\n",
    "def get_fdist_yule(text):\n",
    "    text = tokenize_no_symbols(text)\n",
    "    fdist = word_freq_single(text)\n",
    "    return fdist\n",
    "        \n",
    "def get_num_unique_words(text):\n",
    "    text = tokenize_no_symbols(text.lower())\n",
    "    return len(set(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Implementation below from: https://swizec.com/blog/measuring-vocabulary-richness-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from itertools import groupby\n",
    "\n",
    "def words(entry):\n",
    "    return filter(lambda w: len(w) > 0,\n",
    "                  [w.strip(\"0123456789!:,.?(){}[]\") for w in entry.split()])\n",
    "\n",
    "def yule(entry):\n",
    "    # yule's I measure (the inverse of yule's K measure)\n",
    "    # higher number is higher diversity - richer vocabulary\n",
    "    d = {}\n",
    "    stemmer = PorterStemmer()\n",
    "    for w in words(entry):\n",
    "        w = stemmer.stem(w).lower()\n",
    "        try:\n",
    "            d[w] += 1\n",
    "        except KeyError:\n",
    "            d[w] = 1\n",
    "\n",
    "    M1 = float(len(d))\n",
    "    M2 = sum([len(list(g))*(freq**2) for freq,g in groupby(sorted(d.values()))])\n",
    "\n",
    "    try:\n",
    "        return (M1*M1)/(M2-M1)\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1578/1578 [06:39<00:00,  3.95it/s]\n"
     ]
    }
   ],
   "source": [
    "yules_i_feature = []\n",
    "\n",
    "for i in tqdm(range(len(text_pairs))):\n",
    "    yules_i = yule(corpus[2*i]) - yule(corpus[2*i+1]) #just changed 2*1 to 2*i - rerun for better feature. \n",
    "    yules_i_feature.append(np.abs(yules_i))\n",
    "    \n",
    "yules_i_feature = np.vstack(yules_i_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misspellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spellchecker import SpellChecker\n",
    "\n",
    "# def misspelled_words(text):\n",
    "#     #Library for spell checking\n",
    "#     spell = SpellChecker()\n",
    "#     text = remove_symbols(text)\n",
    "#     #Regex for finding digits\n",
    "#     _digits = re.compile('\\d')\n",
    "\n",
    "#     #List of misspelled words\n",
    "#     misspelled = spell.unknown(text.split())\n",
    "#     #Remove words, that start with capital letter (Likely names)\n",
    "#     no_names = [x for x in misspelled if x.title() not in text]\n",
    "#     #Remove words that contain digits (7th)\n",
    "#     no_digits = [x for x in no_names if not bool(_digits.search(x))]\n",
    "    \n",
    "#     #Find corrections for misspelled words - if word is more than a single character.\n",
    "#     corrections = [spell.correction(x) for x in no_digits if len(x)>1]\n",
    "#     #Remove corrections, if they have no correction (likely misclassified spelling mistake)\n",
    "#     remove_no_correction = [x for x in corrections if x not in misspelled]\n",
    "#     return remove_no_correction\n",
    "\n",
    "# misspelled_words(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misspellings_feature = []\n",
    "\n",
    "# for i in tqdm(range(len(text_pairs))):\n",
    "#     num_of_misspellings = len(misspelled_words(corpus[2*1])) - len(misspelled_words(corpus[2*i+1]))\n",
    "#     misspellings_feature.append(np.abs(num_of_misspellings))\n",
    "    \n",
    "# misspellings_feature = np.vstack(misspellings_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_character_ngram(n, corpus, text_pairs):\n",
    "    #Will return matrix where each row is a pair of texts\n",
    "    vectorizer = TfidfVectorizer(max_features=3000, analyzer='char',ngram_range=(n, n))\n",
    "    print(\"training vectorizer...\",rand_emot())\n",
    "    char_ngram = vectorizer.fit_transform(corpus).toarray()\n",
    "    print(\"vectorizer fit!\", rand_emot())\n",
    "    \n",
    "    feature = []\n",
    "    \n",
    "    for i in tqdm(range(len(text_pairs))):\n",
    "        cos_sim = cosine_sim(char_ngram[2*i], char_ngram[2*i+1])\n",
    "        feature.append(cos_sim)\n",
    "    \n",
    "    feature = np.vstack(feature)\n",
    "    \n",
    "    \n",
    "    return char_ngram, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training vectorizer... ( ◡́.◡̀)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 1578/1578 [00:00<00:00, 26314.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer fit! x)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "character_ngram_matrix, char_bigram_feature = create_character_ngram(2, corpus, text_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97487565],\n",
       "       [0.92723132],\n",
       "       [0.94037656],\n",
       "       ...,\n",
       "       [0.96440691],\n",
       "       [0.9299618 ],\n",
       "       [0.91763796]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_bigram_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stolen from https://stackoverflow.com/questions/31847682/how-to-compute-skipgrams-in-python\n",
    "from itertools import chain, combinations\n",
    "import copy\n",
    "from nltk.util import ngrams\n",
    "\n",
    "corpus_tokens = [nltk.word_tokenize(x) for x in corpus]\n",
    "\n",
    "def pad_sequence(sequence, n, pad_left=False, pad_right=False, pad_symbol=None):\n",
    "    if pad_left:\n",
    "        sequence = chain((pad_symbol,) * (n-1), sequence)\n",
    "    if pad_right:\n",
    "        sequence = chain(sequence, (pad_symbol,) * (n-1))\n",
    "    return sequence\n",
    "\n",
    "def skipgrams(sequence, n, k, pad_left=False, pad_right=False, pad_symbol=None):\n",
    "    sequence_length = len(sequence)\n",
    "    sequence = iter(sequence)\n",
    "    sequence = pad_sequence(sequence, n, pad_left, pad_right, pad_symbol)\n",
    "\n",
    "    if sequence_length + pad_left + pad_right < k:\n",
    "        raise Exception(\"The length of sentence + padding(s) < skip\")\n",
    "\n",
    "    if n < k:\n",
    "        raise Exception(\"Degree of Ngrams (n) needs to be bigger than skip (k)\")    \n",
    "\n",
    "    history = []\n",
    "    nk = n+k\n",
    "\n",
    "    # Return point for recursion.\n",
    "    if nk < 1: \n",
    "        return\n",
    "    # If n+k longer than sequence, reduce k by 1 and recur\n",
    "    elif nk > sequence_length: \n",
    "        for ng in skipgrams(list(sequence), n, k-1):\n",
    "            yield ng\n",
    "\n",
    "    while nk > 1: # Collects the first instance of n+k length history\n",
    "        history.append(next(sequence))\n",
    "        nk -= 1\n",
    "\n",
    "    # Iterative drop first item in history and picks up the next\n",
    "    # while yielding skipgrams for each iteration.\n",
    "    for item in sequence:\n",
    "        history.append(item)\n",
    "        current_token = history.pop(0)      \n",
    "        # Iterates through the rest of the history and \n",
    "        # pick out all combinations the n-1grams\n",
    "        for idx in list(combinations(range(len(history)), n-1)):\n",
    "            ng = [current_token]\n",
    "            for _id in idx:\n",
    "                ng.append(history[_id])\n",
    "            yield tuple(ng)\n",
    "\n",
    "    # Recursively yield the skigrams for the rest of seqeunce where\n",
    "    # len(sequence) < n+k\n",
    "    for ng in list(skipgrams(history, n, k-1)):\n",
    "        yield ng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_features(feature_dict):\n",
    "    '''Save the updated feature dictionary. Takes dictionary as input and saves as binary file\n",
    "    \n",
    "    example: \n",
    "    >>> my_featues = {'freqdist': [1,6,3,5]}\n",
    "    >>> save_features(my_features)'''\n",
    "    \n",
    "    with open('data/features.dat', 'wb') as file:\n",
    "        pickle.dump(feature_dict, file)\n",
    "    print(\"Features saved! :-)\")\n",
    "\n",
    "def load_features():\n",
    "    '''Load feature dictionary. Returns the saved feature as a dictionary.\n",
    "    Will then print all the available features.\n",
    "    \n",
    "    example: \n",
    "    >>> my_features = load_features()'''\n",
    "    \n",
    "    with open('data/features.dat', 'rb') as file:\n",
    "        feats = pickle.load(file)\n",
    "    print(\"Features available:\")\n",
    "    for i in feats.keys():\n",
    "        print(i)\n",
    "    \n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feats = {\n",
    "#     \"Unique words / total no. tokens\":richness_feature, \n",
    "#     \"Punctuation / total no. tokens\": punct_proportion_feature, \n",
    "#     \"Function words freq. dist\": function_words_feature, \n",
    "#     \"Profanity freq. dist.\": profanity_feature,\n",
    "#     \"Function words / total no. tokens\": fw_proportion_feature,\n",
    "#     \"Average word length\": word_len_feature, \n",
    "#     \"Average sentence length\": sent_len_feature, \n",
    "#     \"Character bigrams\": char_bigram_feature, \n",
    "#     \"Yules I\": yules_i_feature, \n",
    "#     \"LIX\": lix_feature\n",
    "    \n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1578"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Order: fw distribution, profanity distribution, character bigram dist., unique words, punctuation proportion, word len, sent len, yules, lix\n",
    "features = function_words_feature\n",
    "features = np.hstack((features,profanity_feature))\n",
    "features = np.hstack((features,char_bigram_feature))\n",
    "features = np.hstack((features,richness_feature))\n",
    "features = np.hstack((features,punct_proportion_feature))\n",
    "features = np.hstack((features,fw_proportion_feature))\n",
    "features = np.hstack((features,word_len_feature))\n",
    "features = np.hstack((features,sent_len_feature))\n",
    "features = np.hstack((features,yules_i_feature))\n",
    "features = np.hstack((features,lix_feature))\n",
    "\n",
    "len(features[:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.dump(\"feature_matrix.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Order: fw distribution, profanity distribution, character bigram dist., unique words, punctuation proportion, word len, sent len, yules, lix\n",
    "feat_matrix = np.load(\"feature_matrix.dat\", allow_pickle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92239211, 0.03173604, 0.97487565, ..., 0.57456802, 7.08319174,\n",
       "        3.93799665],\n",
       "       [0.75270986, 0.19542507, 0.92723132, ..., 2.14250999, 2.18477903,\n",
       "        3.4817081 ],\n",
       "       [0.92438127, 0.        , 0.94037656, ..., 0.68910175, 0.02321739,\n",
       "        6.82064284],\n",
       "       ...,\n",
       "       [0.92556229, 0.        , 0.96440691, ..., 2.91769646, 5.55211268,\n",
       "        2.76283631],\n",
       "       [0.88020293, 0.        , 0.9299618 , ..., 3.09249311, 0.4328907 ,\n",
       "        6.5335426 ],\n",
       "       [0.7391699 , 0.        , 0.91763796, ..., 1.54155921, 2.08026352,\n",
       "        2.29202822]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_scalars = preprocessing.normalize(feat_matrix[:,3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.48243305e-03, 1.39945661e-03, 5.66049004e-03, ...,\n",
       "        7.07165896e-02, 8.71783918e-01, 4.84680109e-01],\n",
       "       [1.58161882e-02, 3.43059535e-04, 3.25301484e-03, ...,\n",
       "        4.61011045e-01, 4.70106216e-01, 7.49170784e-01],\n",
       "       [5.20178098e-03, 6.84528522e-04, 2.08568491e-03, ...,\n",
       "        1.00447669e-01, 3.38430795e-03, 9.94218452e-01],\n",
       "       ...,\n",
       "       [2.33274659e-03, 1.67510033e-03, 7.12070565e-03, ...,\n",
       "        4.25660551e-01, 8.09993560e-01, 4.03068120e-01],\n",
       "       [5.08806851e-03, 4.59425409e-03, 1.24115246e-02, ...,\n",
       "        4.26917583e-01, 5.97604087e-02, 9.01953250e-01],\n",
       "       [4.10044393e-04, 1.37274598e-03, 9.48787532e-04, ...,\n",
       "        4.44507406e-01, 5.99842377e-01, 6.60904566e-01]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.hstack((feat_matrix[:,:2],normalized_scalars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feat_matrix, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = svm_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7056962025316456"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_clf = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = random_forest_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.22433032, 0.02607301, 0.14780122, 0.12592579, 0.08369665,\n",
       "       0.091504  , 0.08243689, 0.0804171 , 0.07452721, 0.0632878 ])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7341772151898734"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, prediction) #random_state decreased the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on feature combinations: (not current)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When train/test split is 80/20\n",
    "\n",
    "Features = function words, profanity words, avg sentence length, avg word length, lix, yules i, number of misspellings difference\n",
    "\n",
    "\n",
    "**All:** \n",
    "    SVM acc = **0.617**0886075949367,\n",
    "    RF acc = **0.724**6835443037974 \n",
    "\n",
    "**Without number of misspellings difference:** \n",
    "    SVM acc = **0.648**7341772151899,\n",
    "    RF acc = **0.721**5189873417721\n",
    "    \n",
    "**Without lix:** \n",
    "    SVM acc = **0.655**0632911392406,\n",
    "    RF acc = **0.727**8481012658228\n",
    "    \n",
    "**Without yules I:** \n",
    "    SVM acc = **0.645**5696202531646,\n",
    "    RF acc = **0.712**0253164556962\n",
    "\n",
    "**Without average sentence length:**\n",
    "    SVM acc = **0.563**2911392405063,\n",
    "    RF acc = **0.718**3544303797469\n",
    "\n",
    "**Without average word length:**\n",
    "    SVM acc = **0.642**4050632911392,\n",
    "    RF acc = **0.674**0506329113924\n",
    "    \n",
    "**Without function words:** \n",
    "    SVM acc = **0.645**5696202531646,\n",
    "    RF acc = **0.648**7341772151899\n",
    "    \n",
    "**Without profanity words:** \n",
    "    SVM acc = **0.645**5696202531646,\n",
    "    RF acc = **0.699**3670886075949 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[1426]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
