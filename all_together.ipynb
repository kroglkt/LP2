{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_emot():\n",
    "    e = [\"(o_o)\",\":-)\",\":P\",\":D\",\"x)\",\"ᓚᘏᗢ\",\"╯°□°）╯︵ ┻━┻\",\":)\",\n",
    "         \"*<:-)\",\"^_^\",\"(⌐■_■)\",\"¯\\_(ツ)_/¯\", \"(T_T)\",\":o\",\"OwO\",\n",
    "        \"( ͡❛ ͜ʖ ͡❛)\",\"(̶◉͛‿◉̶)\",\"( ≖.≖)\",\"(ㆆ_ㆆ)\",\"ʕ•́ᴥ•̀ʔっ\",\"( ◡́.◡̀)\",\"(^◡^ )\"]\n",
    "    return random.choice(e)\n",
    "\n",
    "def load_files():\n",
    "    text_pairs = [] #Would be nice to have as np.array\n",
    "    labels = []\n",
    "    fandom = []\n",
    "    \n",
    "    pair_id = []\n",
    "    true_id = []\n",
    "    \n",
    "    #Load truth JSON\n",
    "    for line in open('data/modified/train_truth.jsonl'):\n",
    "        d = json.loads(line.strip())\n",
    "        labels.append(int(d['same']))\n",
    "        true_id.append(d['id'])\n",
    "\n",
    "    #Load actual fanfic.\n",
    "    print(\"loading fanfic...\",rand_emot())\n",
    "    for line in tqdm(open('data/modified/train_pair.jsonl')):\n",
    "        d = json.loads(line.strip())\n",
    "        text_pairs.append(d['pair'])\n",
    "        fandom.append(d['fandoms'])\n",
    "        pair_id.append(d['id'])\n",
    "\n",
    "    print(\"done loading\",rand_emot())\n",
    "    \n",
    "    return text_pairs, labels, fandom, pair_id, true_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "173it [00:00, 1717.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading fanfic... :-)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1578it [00:00, 1669.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading (̶◉͛‿◉̶)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_pairs, labels, fandom, pair_id, true_id = load_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word frequency and word frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_distribution(text_pair): #expect untokenized input\n",
    "    \n",
    "    pair = []\n",
    "    \n",
    "    for text in text_pair: \n",
    "        tokens = nltk.word_tokenize(text) #tokenize\n",
    "        \n",
    "        freq_dist = nltk.FreqDist(tokens) #compute frequency distribution\n",
    "        pair.append(freq_dist)\n",
    "        \n",
    "    return pair #return frequency distribution of each fanfic in the input pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq(text_pair): #expects tokenized pairs\n",
    "    fdist0 = nltk.FreqDist(text_pair[0])\n",
    "    fdist1 = nltk.FreqDist(text_pair[1])\n",
    "    \n",
    "    return [fdist0, fdist1]\n",
    "\n",
    "def word_freq_single(text):\n",
    "    fdist = nltk.FreqDist(text)\n",
    "    return fdist\n",
    "\n",
    "def tokenize(text_pair):\n",
    "    return [nltk.word_tokenize(text_pair[0]),nltk.word_tokenize(text_pair[1])]\n",
    "\n",
    "def vector_freq_dist(freq_dists): #I don't think this works...\n",
    "    return [list(freq_dists[0].values()), list(freq_dists[1].values())]\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(text_pairs):\n",
    "    '''input all text pairs to create a corpus'''\n",
    "    corpus = [x[i] for x in text_pairs for i in range(len(x))]\n",
    "    return corpus\n",
    "\n",
    "def fit_tfidf(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    print(\"training vectorizer...\",rand_emot())\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    print(\"vectorizer fit!\", rand_emot())\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(X[0].T.todense(), index=vectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "    df = df.sort_values('TF-IDF', ascending=False)\n",
    "    \n",
    "    return X, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = create_corpus(text_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training vectorizer... (^◡^ )\n",
      "vectorizer fit! (̶◉͛‿◉̶)\n"
     ]
    }
   ],
   "source": [
    "#tf-idf on the raw text. Likely not useful, as you can see, it is sesnitive to the fandom.\n",
    "raw_tfidf, tfidf_df = fit_tfidf(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TF-IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.389299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.295670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kuroko</th>\n",
       "      <td>0.283424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>judgement</th>\n",
       "      <td>0.247891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>0.184794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.182388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>0.182330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>0.172474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>she</th>\n",
       "      <td>0.169618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>her</th>\n",
       "      <td>0.146488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             TF-IDF\n",
       "the        0.389299\n",
       "to         0.295670\n",
       "kuroko     0.283424\n",
       "judgement  0.247891\n",
       "was        0.184794\n",
       "and        0.182388\n",
       "it         0.182330\n",
       "that       0.172474\n",
       "she        0.169618\n",
       "her        0.146488"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempting to perform tf-idf on only symbols.\n",
    "def isolate_symbols(corpus):\n",
    "    #Add \\d to omit digits too.\n",
    "    sym_corpus = []\n",
    "    for text in corpus:\n",
    "        sym_corpus.append(' '.join(re.findall(\"[^a-zA-Z\\s]+\", text)))\n",
    "    return sym_corpus\n",
    "\n",
    "symbols = isolate_symbols(corpus)\n",
    "\n",
    "#Okay, tf-idf doesn't work with symbols. I'll convert them to made-up words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS-tagging and Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## POS Tagging and ngrams\n",
    "tokens = nltk.word_tokenize(corpus[0])\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "pos_bigrams = nltk.bigrams(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIX calculation - LIX = readability index, a measure for the readability of a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lix(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    splt = text.split()\n",
    "    o = len(splt)+1\n",
    "    p = len([x for x in tokens if x=='.'])+1\n",
    "    l = len([x for x in tokens if len(x)>6])+1\n",
    "    \n",
    "    return (o/p)+((l*100)/o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.910954831914932\n",
      "30.709476503232075\n",
      "34.64747315241397\n",
      "31.16576505295295\n",
      "35.736950980754315\n",
      "27.826830313919487\n",
      "23.657931484369428\n",
      "30.435502148277408\n",
      "29.67444070920108\n",
      "27.62315961549868\n"
     ]
    }
   ],
   "source": [
    "for text in corpus[:10]:\n",
    "    print(compute_lix(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence and word length - compute sentence and word length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 7, 10, 17,  7, 13,  3,  3, 10, 20,  2,  7,  4,  3, 13, 16, 17, 27,\n",
       "         6, 13, 20,  1, 22, 20, 15, 14, 12,  7, 14, 15,  7,  1,  9, 15, 21,\n",
       "         5,  2, 14,  3, 23,  8, 11,  1,  6, 12,  9,  8,  8,  3,  9, 14,  1,\n",
       "         8,  0,  2,  3, 14,  6,  2,  3,  8,  2,  2, 12,  9, 15,  2, 17, 17,\n",
       "         4, 14,  8,  7, 23, 11,  3, 18, 23,  6,  7, 23, 24, 34, 10,  2,  2,\n",
       "        12,  2,  5, 11, 14,  4, 12, 16, 17, 12,  2,  3,  3,  2,  6, 24, 26,\n",
       "         1,  3, 23,  2,  3,  3,  2, 26,  2,  5, 23,  3,  7, 15, 29, 22, 25,\n",
       "        11,  4, 10,  8,  4, 33,  5,  7,  7,  8, 20, 27, 15, 18, 13, 10, 12,\n",
       "         4, 13, 17, 11,  2, 13,  9,  2,  5, 11, 20, 29, 19,  0,  3,  2,  4,\n",
       "        14,  1,  9,  3,  8,  0,  2,  8,  5, 19,  8, 17, 21, 17, 16, 21, 26,\n",
       "         9,  5, 32,  6, 13, 15,  3, 17, 29,  0,  4, 10,  1,  7,  3,  2,  4,\n",
       "        13,  5,  0,  1, 14,  3, 24, 10,  1,  3,  2, 18,  1,  4,  2, 18,  5,\n",
       "        25,  2,  5,  4,  2, 25,  8,  1,  8,  5,  6, 10,  2,  1,  2, 10, 19,\n",
       "         0,  1,  9, 21,  4,  6,  3,  2,  4, 10, 11,  9, 10,  6,  1,  9, 13,\n",
       "         3,  2,  7,  9, 32,  5, 11, 23, 32, 11,  3,  9, 14,  4, 15, 11,  1,\n",
       "        16,  4,  1,  2,  5,  2,  5,  9,  9,  4,  7, 10, 23, 23, 27, 11, 30,\n",
       "        25, 17, 20, 14,  8, 13,  7, 21, 33, 10,  1,  1,  1, 21, 11,  4, 12,\n",
       "        12, 26,  6, 18, 22,  7,  3, 16, 22,  1, 17,  2,  1, 10,  3, 14,  4,\n",
       "         3,  2,  3, 10, 12, 12,  1,  5,  2, 10,  3,  6,  5, 12,  0,  7,  1,\n",
       "         1,  1,  7,  6, 35,  4,  8,  3,  2, 14,  1,  8,  2,  1, 25, 11, 19,\n",
       "         9,  5,  1, 29,  6,  7, 23,  5, 19,  1,  3, 25,  2,  5, 11,  7,  9,\n",
       "         4, 20,  3,  3,  7, 17, 35, 22,  6, 28, 17, 25, 15, 29, 24, 28,  1,\n",
       "         3,  2, 11, 21,  5,  7, 17,  7,  6]),\n",
       " array([2, 3, 3, ..., 3, 6, 6]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_symbols(text):\n",
    "    sentences = re.split('[\\.+|!|?]', text)\n",
    "    sentences = [re.sub(r\"[^\\w]+\", ' ', x) for x in sentences if len(x.strip()) != 0]\n",
    "    return ' '.join(sentences)\n",
    "\n",
    "def get_sent_word_length(text):\n",
    "    #Function, which removes symbols and count words in sentence\n",
    "    #Output: length of each sentence & length of each word\n",
    "    sentences = re.split('[\\.+|!|?]', text)\n",
    "    sentences = [re.sub(r\"[^\\w]+\", ' ', x) for x in sentences if len(x.strip()) != 0]\n",
    "    word_sentences = [nltk.word_tokenize(x) for x in sentences]\n",
    "    sentence_lengths = np.array([len(x) for x in word_sentences])\n",
    "    word_lengths = np.array([len(s) for x in word_sentences for s in x])\n",
    "    return sentence_lengths, word_lengths\n",
    "\n",
    "get_sent_word_length(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolating function words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'data/modified/train_pair.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/function_words_clean.txt', \"r\") as fw:\n",
    "    func_words = fw.read().split()\n",
    "\n",
    "def isolate_fw(data, f_words): #data must be json file - input must be path to file \n",
    "    fw_in_data = []\n",
    "\n",
    "    for line in tqdm(open(data)):\n",
    "        function_words = []\n",
    "        d = json.loads(line.strip()) #load the json file\n",
    "        text = d.get(\"pair\") #get the actual fanfic\n",
    "        words = text[0].split() #split fanfic into words in list\n",
    "        for word in words: \n",
    "            if word in f_words: #if the word is a function word\n",
    "                function_words.append(word)\n",
    "                \n",
    "        stringed_function_words = \" \".join(function_words)\n",
    "        \n",
    "        #append all function words as one long string in a list\n",
    "        fw_in_data.append([stringed_function_words]) #fw_in_data is a list with lists\n",
    "        #each list contains a string of all function words for each pair\n",
    "        #should it be a string for each pair?\n",
    "        \n",
    "    return fw_in_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1578it [00:21, 75.10it/s]\n"
     ]
    }
   ],
   "source": [
    "test = isolate_fw(data, func_words) #why is this so fast?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to simplifying isolation of function words - make it take text_pairs as input so we only load the files once\n",
    "# and use the same kind of input in our various functions\n",
    "\n",
    "#this made it hella slow - fix it!\n",
    "\n",
    "def isolate_fw_2(data, f_words): #data must be the text_pairs from load_files()\n",
    "    fw_in_data = []\n",
    "    \n",
    "    fw_text_pairs = []\n",
    "    for pair in tqdm(data):\n",
    "        fw_text_pairs = []\n",
    "        for text in pair: \n",
    "            function_words = []\n",
    "            \n",
    "            words = text.split() #split fanfic into words in list\n",
    "            \n",
    "            for word in words: \n",
    "                if word in f_words: #if the word is a function word\n",
    "                    function_words.append(word)\n",
    "                \n",
    "            stringed_function_words = \" \".join(function_words) #for each fanfic in a pair, makes FW a long string. \n",
    "            fw_text_pairs.append(stringed_function_words) \n",
    "            \n",
    "        #append text pairs with only their function words\n",
    "        fw_in_data.append(fw_text_pairs) \n",
    "        \n",
    "    return fw_in_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1578/1578 [00:33<00:00, 47.12it/s]\n"
     ]
    }
   ],
   "source": [
    "fw_inData_2 = isolate_fw_2(text_pairs, func_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FreqDist({'the': 134, 'to': 118, 'a': 91, 'was': 72, 'and': 64, 'of': 58, 'in': 53, 'her': 45, 'that': 43, 'for': 38, ...}),\n",
       " FreqDist({'the': 111, 'and': 102, 'to': 98, 'a': 61, 'her': 56, 'his': 47, 'of': 45, 'she': 42, 'in': 41, 'he': 37, ...})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_distribution(fw_inData_2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolating profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'data/modified/train_pair.jsonl'\n",
    "with open('data/profanity_words_clean.txt', \"r\") as pr:\n",
    "    prof_words = pr.read().split()\n",
    "del prof_words[:4]\n",
    "del prof_words[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'data/modified/train_pair.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_profanity(data, prof_words):\n",
    "    profanity = []\n",
    "    \n",
    "    for pair in tqdm(data):\n",
    "        profanity_pairs = []\n",
    "        for text in pair:\n",
    "            resultwords = []\n",
    "\n",
    "            #d = json.loads(line.strip())\n",
    "            #text = d.get(\"pair\") \n",
    "            words = text.split() \n",
    "\n",
    "            resultwords  = [word for word in words if word.lower() in prof_words]\n",
    "\n",
    "            result = \" \".join(resultwords)\n",
    "            profanity_pairs.append(result)\n",
    "        \n",
    "        profanity.append(profanity_pairs) \n",
    "\n",
    "        \n",
    "    return profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1578/1578 [01:05<00:00, 24.27it/s]\n"
     ]
    }
   ],
   "source": [
    "profanity_inData = isolate_profanity(text_pairs, prof_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FreqDist({'balls': 5, 'bastard': 1, 'hell': 1}),\n",
       " FreqDist({'bloody': 2, 'shit': 2, 'hell': 2, 'fucking': 2})]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_distribution(profanity_inData[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yule's K computations - different implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Our own implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_no_symbols(text):\n",
    "    return nltk.word_tokenize(re.sub(r'[^\\w]', ' ', text))\n",
    "\n",
    "def get_fdist_yule(text):\n",
    "    text = tokenize_no_symbols(text)\n",
    "    fdist = word_freq_single(text)\n",
    "    return fdist\n",
    "        \n",
    "def get_num_unique_words(text):\n",
    "    text = tokenize_no_symbols(text.lower())\n",
    "    return len(set(text))\n",
    "\n",
    "#Remove this function. It is incorrectly implemented.\n",
    "def yules_k(text):\n",
    "    C = 10000\n",
    "    splt = text.split()\n",
    "    N = len(splt)\n",
    "    Vn = get_num_unique_words(text)\n",
    "    fdist = get_fdist_yule(text)\n",
    "    max_word = fdist.most_common()[0][1]\n",
    "    \n",
    "    var = 0\n",
    "    \n",
    "    for m in range(max_word):\n",
    "        Vmn = len([x for x in fdist if fdist[x]==m])\n",
    "        mNs = np.power(m/N,2)\n",
    "        \n",
    "        var += Vmn*mNs\n",
    "    \n",
    "    return C*(-1/N+var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.62083541967105 29.910954831914932 24.70988058775612\n",
      "50.06509040692571 30.709476503232075 19.355613903693637\n",
      "68.87825528810403 34.64747315241397 34.230782135690056\n",
      "69.87291123035095 31.16576505295295 38.707146177398\n",
      "57.89319891883996 35.736950980754315 22.15624793808564\n",
      "67.16115024294662 27.826830313919487 39.33431992902713\n",
      "63.76863861014663 23.657931484369428 40.1107071257772\n",
      "85.34784357474064 30.435502148277408 54.91234142646323\n",
      "52.83657559952693 29.67444070920108 23.162134890325852\n",
      "59.121650604821426 27.62315961549868 31.498490989322747\n"
     ]
    }
   ],
   "source": [
    "for i in corpus[:10]:\n",
    "    k,lix = yules_k(i), compute_lix(i)\n",
    "    print(k, lix, k-lix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.62083541967105\n",
      "50.06509040692571\n",
      "68.87825528810403\n",
      "69.87291123035095\n",
      "57.89319891883996\n",
      "67.16115024294662\n",
      "63.76863861014663\n",
      "85.34784357474064\n",
      "52.83657559952693\n",
      "59.121650604821426\n"
     ]
    }
   ],
   "source": [
    "for i in corpus[:10]:\n",
    "    k = yules_k(i)\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Implementation below from:\n",
    "https://gist.github.com/magnusnissel/d9521cb78b9ae0b2c7d6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "\n",
    "def tokenize(s):\n",
    "    tokens = re.split(r\"[^0-9A-Za-z\\-'_]+\", s)\n",
    "    return tokens\n",
    "\n",
    "def get_yules(s):\n",
    "    \"\"\" \n",
    "    Returns a tuple with Yule's K and Yule's I.\n",
    "    (cf. Oakes, M.P. 1998. Statistics for Corpus Linguistics.\n",
    "    International Journal of Applied Linguistics, Vol 10 Issue 2)\n",
    "    In production this needs exception handling.\n",
    "    \"\"\"\n",
    "    tokens = tokenize(s)\n",
    "    token_counter = collections.Counter(tok.upper() for tok in tokens)\n",
    "    m1 = sum(token_counter.values())\n",
    "    m2 = sum([freq ** 2 for freq in token_counter.values()])\n",
    "    i = (m1*m1) / (m2-m1)\n",
    "    k = 1/i * 10000\n",
    "    return (k, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76.24848330128657, 131.15014970837154)\n",
      "(66.47285249322603, 150.4373533694685)\n",
      "(93.3293041366282, 107.1474826959026)\n",
      "(106.93616216764885, 93.51373564653021)\n",
      "(109.14087031159232, 91.62470458088198)\n",
      "(91.70043097696845, 109.05074156643394)\n",
      "(83.6476796371318, 119.54904240476895)\n",
      "(94.47986224640348, 105.8426606711174)\n",
      "(83.97021309490067, 119.08984902418068)\n",
      "(88.22340444450815, 113.34860701606596)\n"
     ]
    }
   ],
   "source": [
    "for text in corpus[:10]: \n",
    "    k_i = get_yules(text)\n",
    "    print(k_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Implementation below from: https://swizec.com/blog/measuring-vocabulary-richness-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from itertools import groupby\n",
    "\n",
    "def words(entry):\n",
    "    return filter(lambda w: len(w) > 0,\n",
    "                  [w.strip(\"0123456789!:,.?(){}[]\") for w in entry.split()])\n",
    "\n",
    "def yule(entry):\n",
    "    # yule's I measure (the inverse of yule's K measure)\n",
    "    # higher number is higher diversity - richer vocabulary\n",
    "    d = {}\n",
    "    stemmer = PorterStemmer()\n",
    "    for w in words(entry):\n",
    "        w = stemmer.stem(w).lower()\n",
    "        try:\n",
    "            d[w] += 1\n",
    "        except KeyError:\n",
    "            d[w] = 1\n",
    "\n",
    "    M1 = float(len(d))\n",
    "    M2 = sum([len(list(g))*(freq**2) for freq,g in groupby(sorted(d.values()))])\n",
    "\n",
    "    try:\n",
    "        return (M1*M1)/(M2-M1)\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.645130247537017\n",
      "17.728321988754068\n",
      "6.742396307162275\n",
      "8.927175336111278\n",
      "7.337997328744253\n",
      "7.3147799405225005\n",
      "8.430640063377464\n",
      "11.789875705444068\n",
      "9.429946152509988\n",
      "10.677045985641614\n"
     ]
    }
   ],
   "source": [
    "for text in corpus[:10]: \n",
    "    yules_i = yule(text) #yules_i = inverse of yules K  - why do you want the inverse instead? \n",
    "    print(yules_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<filter object at 0x0000026DBF325608>\n",
      "<filter object at 0x0000026DBF341B48>\n"
     ]
    }
   ],
   "source": [
    "for text in corpus[:2]: \n",
    "    tokenized_words = words(text)  \n",
    "    print(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misspellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fastball',\n",
       " 'cooked',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'voiceover',\n",
       " 'he',\n",
       " 'peace',\n",
       " 'sis',\n",
       " 'cancer',\n",
       " 'tad',\n",
       " 'threatened',\n",
       " 'nonstop',\n",
       " 'ooh',\n",
       " 'rank',\n",
       " 'capulets',\n",
       " 'default',\n",
       " 'la',\n",
       " 'herface',\n",
       " 'dizzy',\n",
       " 'pandora',\n",
       " 'ranks',\n",
       " 'mortys',\n",
       " 'encompass']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "def misspelled_words(text):\n",
    "    #Library for spell checking\n",
    "    spell = SpellChecker()\n",
    "    text = remove_symbols(text)\n",
    "    #Regex for finding digits\n",
    "    _digits = re.compile('\\d')\n",
    "\n",
    "    #List of misspelled words\n",
    "    misspelled = spell.unknown(text.split())\n",
    "    #Remove words, that start with capital letter (Likely names)\n",
    "    no_names = [x for x in misspelled if x.title() not in text]\n",
    "    #Remove words that contain digits (7th)\n",
    "    no_digits = [x for x in no_names if not bool(_digits.search(x))]\n",
    "    \n",
    "    #Find corrections for misspelled words - if word is more than a single character.\n",
    "    corrections = [spell.correction(x) for x in no_digits if len(x)>1]\n",
    "    #Remove corrections, if they have no correction (likely misclassified spelling mistake)\n",
    "    remove_no_correction = [x for x in corrections if x not in misspelled]\n",
    "    return remove_no_correction\n",
    "\n",
    "misspelled_words(corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
