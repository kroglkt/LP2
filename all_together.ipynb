{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_emot():\n",
    "    e = [\"(o_o)\",\":-)\",\":P\",\":D\",\"x)\",\"ᓚᘏᗢ\",\"╯°□°）╯︵ ┻━┻\",\":)\",\n",
    "         \"*<:-)\",\"^_^\",\"(⌐■_■)\",\"¯\\_(ツ)_/¯\", \"(T_T)\",\":o\",\"OwO\",\n",
    "        \"( ͡❛ ͜ʖ ͡❛)\",\"(̶◉͛‿◉̶)\",\"( ≖.≖)\",\"(ㆆ_ㆆ)\",\"ʕ•́ᴥ•̀ʔっ\",\"( ◡́.◡̀)\",\"(^◡^ )\"]\n",
    "    return random.choice(e)\n",
    "\n",
    "def load_files():\n",
    "    text_pairs = [] #Would be nice to have as np.array\n",
    "    labels = []\n",
    "    fandom = []\n",
    "    \n",
    "    pair_id = []\n",
    "    true_id = []\n",
    "    \n",
    "    #Load truth JSON\n",
    "    for line in open('data/modified/train_truth.jsonl'):\n",
    "        d = json.loads(line.strip())\n",
    "        labels.append(int(d['same']))\n",
    "        true_id.append(d['id'])\n",
    "\n",
    "    #Load actual fanfic.\n",
    "    print(\"loading fanfic...\",rand_emot())\n",
    "    for line in tqdm(open('data/modified/train_pair.jsonl')):\n",
    "        d = json.loads(line.strip())\n",
    "        text_pairs.append(d['pair'])\n",
    "        fandom.append(d['fandoms'])\n",
    "        pair_id.append(d['id'])\n",
    "\n",
    "    print(\"done loading\",rand_emot())\n",
    "    \n",
    "    return text_pairs, labels, fandom, pair_id, true_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "588it [00:00, 2898.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading fanfic... ( ≖.≖)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1578it [00:00, 2886.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading :P\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_pairs, labels, fandom, pair_id, true_id = load_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word frequency and word frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_distribution(text_pair): #expect untokenized input\n",
    "    \n",
    "    pair = []\n",
    "    \n",
    "    for text in text_pair: \n",
    "        tokens = nltk.word_tokenize(text) #tokenize\n",
    "        \n",
    "        freq_dist = nltk.FreqDist(tokens) #compute frequency distribution\n",
    "        pair.append(freq_dist)\n",
    "        \n",
    "    return pair #return frequency distribution of each fanfic in the input pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq(text_pair): #expects tokenized pairs\n",
    "    fdist0 = nltk.FreqDist(text_pair[0])\n",
    "    fdist1 = nltk.FreqDist(text_pair[1])\n",
    "    \n",
    "    return [fdist0, fdist1]\n",
    "\n",
    "def word_freq_single(text):\n",
    "    fdist = nltk.FreqDist(text)\n",
    "    return fdist\n",
    "\n",
    "def tokenize(text_pair):\n",
    "    return [nltk.word_tokenize(text_pair[0]),nltk.word_tokenize(text_pair[1])]\n",
    "\n",
    "def vector_freq_dist(freq_dists): #I don't think this works...\n",
    "    return [list(freq_dists[0].values()), list(freq_dists[1].values())]\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)+0.0000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(text_pairs):\n",
    "    '''input all text pairs to create a corpus'''\n",
    "    corpus = [x[i] for x in text_pairs for i in range(len(x))]\n",
    "    return corpus\n",
    "\n",
    "def fit_tfidf(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    print(\"training vectorizer...\",rand_emot())\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    print(\"vectorizer fit!\", rand_emot())\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(X[0].T.todense(), index=vectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "    df = df.sort_values('TF-IDF', ascending=False)\n",
    "    \n",
    "    return X, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = create_corpus(text_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training vectorizer... (ㆆ_ㆆ)\n",
      "vectorizer fit! (T_T)\n"
     ]
    }
   ],
   "source": [
    "#tf-idf on the raw text. Likely not useful, as you can see, it is sesnitive to the fandom.\n",
    "raw_tfidf, tfidf_df = fit_tfidf(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10x86525 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10515 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_tfidf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TF-IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.389299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.295670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kuroko</th>\n",
       "      <td>0.283424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>judgement</th>\n",
       "      <td>0.247891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>0.184794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.182388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>0.182330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>0.172474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>she</th>\n",
       "      <td>0.169618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>her</th>\n",
       "      <td>0.146488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             TF-IDF\n",
       "the        0.389299\n",
       "to         0.295670\n",
       "kuroko     0.283424\n",
       "judgement  0.247891\n",
       "was        0.184794\n",
       "and        0.182388\n",
       "it         0.182330\n",
       "that       0.172474\n",
       "she        0.169618\n",
       "her        0.146488"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempting to perform tf-idf on only symbols.\n",
    "def isolate_symbols(corpus):\n",
    "    #Add \\d to omit digits too.\n",
    "    sym_corpus = []\n",
    "    for text in corpus:\n",
    "        sym_corpus.append(' '.join(re.findall(\"[^a-zA-Z\\s]+\", text)))\n",
    "    return sym_corpus\n",
    "\n",
    "symbols = isolate_symbols(corpus)\n",
    "\n",
    "#Okay, tf-idf doesn't work with symbols. I'll convert them to made-up words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training vectorizer... (^◡^ )\n",
      "vectorizer fit! *<:-)\n"
     ]
    }
   ],
   "source": [
    "punct_matrix, punct_DF = fit_tfidf(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punct_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS-tagging and Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## POS Tagging and ngrams\n",
    "tokens = nltk.word_tokenize(corpus[0])\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "pos_bigrams = nltk.bigrams(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIX calculation - LIX = readability index, a measure for the readability of a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lix(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    splt = text.split()\n",
    "    o = len(splt)+1\n",
    "    p = len([x for x in tokens if x=='.'])+1\n",
    "    l = len([x for x in tokens if len(x)>6])+1\n",
    "    \n",
    "    return (o/p)+((l*100)/o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text in corpus[:10]:\n",
    "#     print(compute_lix(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lix_feature = []\n",
    "\n",
    "# for i in tqdm(range(len(text_pairs))):\n",
    "#     lix = compute_lix(corpus[2*1]) - compute_lix(corpus[2*i+1])\n",
    "#     lix_feature.append(np.abs(lix))\n",
    "    \n",
    "# lix_feature = np.vstack(lix_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lix_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence and word length - compute sentence and word length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols(text):\n",
    "    sentences = re.split('[\\.+|!|?]', text)\n",
    "    sentences = [re.sub(r\"[^\\w]+\", ' ', x) for x in sentences if len(x.strip()) != 0]\n",
    "    return ' '.join(sentences)\n",
    "\n",
    "def get_sent_word_length(text):\n",
    "    #Function, which removes symbols and count words in sentence\n",
    "    #Output: length of each sentence & length of each word\n",
    "    sentences = re.split('[\\.+|!|?]', text)\n",
    "    sentences = [re.sub(r\"[^\\w]+\", ' ', x) for x in sentences if len(x.strip()) != 0]\n",
    "    word_sentences = [nltk.word_tokenize(x) for x in sentences]\n",
    "    sentence_lengths = np.array([len(x) for x in word_sentences])\n",
    "    word_lengths = np.array([len(s) for x in word_sentences for s in x])\n",
    "    return sentence_lengths, word_lengths\n",
    "\n",
    "# get_sent_word_length(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sent_len = []\n",
    "avg_word_len = []\n",
    "\n",
    "# for i in tqdm(range(len(text_pairs))):\n",
    "#     sent_length1, word_lengths1 = get_sent_word_length(corpus[i*2])\n",
    "#     sent_length2, word_lengths2 = get_sent_word_length(corpus[i*2+1])\n",
    "    \n",
    "#     avg_sent = np.average(sent_length1) - np.average(sent_length2)\n",
    "#     avg_word = np.average(word_lengths1) - np.average(word_lengths2)\n",
    "    \n",
    "#     avg_sent_len.append(np.abs(avg_sent))\n",
    "#     avg_word_len.append(np.abs(avg_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_sent_len = np.vstack(avg_sent_len)\n",
    "# avg_word_len = np.vstack(avg_word_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_sent_word_feat = np.hstack((avg_sent_len, avg_word_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolating function words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'data/modified/train_pair.jsonl'\n",
    "\n",
    "with open('data/function_words_clean.txt', \"r\") as fw:\n",
    "    func_words = fw.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def isolate_fw(data, f_words): #data must be json file - input must be path to file \n",
    "#     fw_in_data = []\n",
    "\n",
    "#     for line in tqdm(open(data)):\n",
    "#         function_words = []\n",
    "#         d = json.loads(line.strip()) #load the json file\n",
    "#         text = d.get(\"pair\") #get the actual fanfic\n",
    "#         words = text[0].split() #split fanfic into words in list\n",
    "#         for word in words: \n",
    "#             if word in f_words: #if the word is a function word\n",
    "#                 function_words.append(word)\n",
    "                \n",
    "#         stringed_function_words = \" \".join(function_words)\n",
    "        \n",
    "#         #append all function words as one long string in a list\n",
    "#         fw_in_data.append([stringed_function_words]) #fw_in_data is a list with lists\n",
    "#         #each list contains a string of all function words for each pair\n",
    "#         #should it be a string for each pair?\n",
    "        \n",
    "#     return fw_in_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = isolate_fw(data, func_words) #why is this so fast?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to simplify isolation of function words - make it take text_pairs as input so we only load the files once\n",
    "# and use the same kind of input in our various functions\n",
    "\n",
    "def isolate_fw_2(data, f_words): #data must be the text_pairs from load_files()\n",
    "    fw_in_data = []\n",
    "    \n",
    "    fw_text_pairs = []\n",
    "    for pair in tqdm(data):\n",
    "        fw_text_pairs = []\n",
    "        for text in pair: \n",
    "            function_words = []\n",
    "            \n",
    "            words = text.split() #split fanfic into words in list\n",
    "            \n",
    "            for word in words: \n",
    "                if word in f_words: #if the word is a function word\n",
    "                    function_words.append(word)\n",
    "                \n",
    "            stringed_function_words = \" \".join(function_words) #for each fanfic in a pair, makes FW a long string. \n",
    "            fw_text_pairs.append(stringed_function_words) \n",
    "            \n",
    "        #append text pairs with only their function words\n",
    "        fw_in_data.append(fw_text_pairs) \n",
    "        \n",
    "    return fw_in_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fw_inData_2 = isolate_fw_2(text_pairs, func_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fw_corpus = create_corpus(fw_inData_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FW_matrix, fw_dateframe = fit_tfidf(fw_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fw_matrix = FW_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw_feature = []\n",
    "\n",
    "# for i in range(len(text_pairs)):\n",
    "#     cos_sim = cosine_sim(fw_matrix[2*i], fw_matrix[2*i+1])\n",
    "#     fw_feature.append(cos_sim)\n",
    "# fw_feature = np.vstack(fw_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolating profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'data/modified/train_pair.jsonl'\n",
    "with open('data/profanity_words_clean.txt', \"r\") as pr:\n",
    "    prof_words = pr.read().split()\n",
    "del prof_words[:4]\n",
    "del prof_words[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'data/modified/train_pair.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_profanity(data, prof_words):\n",
    "    profanity = []\n",
    "    \n",
    "    for pair in tqdm(data):\n",
    "        profanity_pairs = []\n",
    "        for text in pair:\n",
    "            resultwords = []\n",
    "\n",
    "            #d = json.loads(line.strip())\n",
    "            #text = d.get(\"pair\") \n",
    "            words = text.split() \n",
    "\n",
    "            resultwords  = [word for word in words if word.lower() in prof_words]\n",
    "\n",
    "            result = \" \".join(resultwords)\n",
    "            profanity_pairs.append(result)\n",
    "        \n",
    "        profanity.append(profanity_pairs) \n",
    "\n",
    "        \n",
    "    return profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profanity_inData = isolate_profanity(text_pairs, prof_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profanity_corpus = create_corpus(profanity_inData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profanity_matrix, profanity_dataframe = fit_tfidf(profanity_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profanity_matrix = profanity_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "profanity_feature = []\n",
    "\n",
    "# for i in range(len(text_pairs)):\n",
    "#     cos_sim = cosine_sim(profanity_matrix[2*i], profanity_matrix[2*i+1])\n",
    "#     profanity_feature.append(cos_sim)\n",
    "# profanity_feature = np.vstack(profanity_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = np.hstack((fw_feature, profanity_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yule's K computations - different implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Our own implementation - delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_no_symbols(text):\n",
    "    return nltk.word_tokenize(re.sub(r'[^\\w]', ' ', text))\n",
    "\n",
    "def get_fdist_yule(text):\n",
    "    text = tokenize_no_symbols(text)\n",
    "    fdist = word_freq_single(text)\n",
    "    return fdist\n",
    "        \n",
    "def get_num_unique_words(text):\n",
    "    text = tokenize_no_symbols(text.lower())\n",
    "    return len(set(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Implementation below from:\n",
    "https://gist.github.com/magnusnissel/d9521cb78b9ae0b2c7d6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "\n",
    "def tokenize(s):\n",
    "    tokens = re.split(r\"[^0-9A-Za-z\\-'_]+\", s)\n",
    "    return tokens\n",
    "\n",
    "def get_yules(s):\n",
    "    \"\"\" \n",
    "    Returns a tuple with Yule's K and Yule's I.\n",
    "    (cf. Oakes, M.P. 1998. Statistics for Corpus Linguistics.\n",
    "    International Journal of Applied Linguistics, Vol 10 Issue 2)\n",
    "    In production this needs exception handling.\n",
    "    \"\"\"\n",
    "    tokens = tokenize(s)\n",
    "    token_counter = collections.Counter(tok.upper() for tok in tokens)\n",
    "    m1 = sum(token_counter.values())\n",
    "    m2 = sum([freq ** 2 for freq in token_counter.values()])\n",
    "    i = (m1*m1) / (m2-m1)\n",
    "    k = 1/i * 10000\n",
    "    return (k, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text in corpus[:10]: \n",
    "#     k_i = get_yules(text)\n",
    "#     print(k_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Implementation below from: https://swizec.com/blog/measuring-vocabulary-richness-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from itertools import groupby\n",
    "\n",
    "def words(entry):\n",
    "    return filter(lambda w: len(w) > 0,\n",
    "                  [w.strip(\"0123456789!:,.?(){}[]\") for w in entry.split()])\n",
    "\n",
    "def yule(entry):\n",
    "    # yule's I measure (the inverse of yule's K measure)\n",
    "    # higher number is higher diversity - richer vocabulary\n",
    "    d = {}\n",
    "    stemmer = PorterStemmer()\n",
    "    for w in words(entry):\n",
    "        w = stemmer.stem(w).lower()\n",
    "        try:\n",
    "            d[w] += 1\n",
    "        except KeyError:\n",
    "            d[w] = 1\n",
    "\n",
    "    M1 = float(len(d))\n",
    "    M2 = sum([len(list(g))*(freq**2) for freq,g in groupby(sorted(d.values()))])\n",
    "\n",
    "    try:\n",
    "        return (M1*M1)/(M2-M1)\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text in corpus[:10]: \n",
    "#     yules_i = yule(text) #yules_i = inverse of yules K  - why do you want the inverse instead? \n",
    "#     print(yules_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "yules_i_feature = []\n",
    "\n",
    "# for i in tqdm(range(len(text_pairs))):\n",
    "#     yules_i = yule(corpus[2*1]) - yule(corpus[2*i+1])\n",
    "#     yules_i_feature.append(np.abs(yules_i))\n",
    "    \n",
    "# yules_i_feature = np.vstack(yules_i_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misspellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "def misspelled_words(text):\n",
    "    #Library for spell checking\n",
    "    spell = SpellChecker()\n",
    "    text = remove_symbols(text)\n",
    "    #Regex for finding digits\n",
    "    _digits = re.compile('\\d')\n",
    "\n",
    "    #List of misspelled words\n",
    "    misspelled = spell.unknown(text.split())\n",
    "    #Remove words, that start with capital letter (Likely names)\n",
    "    no_names = [x for x in misspelled if x.title() not in text]\n",
    "    #Remove words that contain digits (7th)\n",
    "    no_digits = [x for x in no_names if not bool(_digits.search(x))]\n",
    "    \n",
    "    #Find corrections for misspelled words - if word is more than a single character.\n",
    "    corrections = [spell.correction(x) for x in no_digits if len(x)>1]\n",
    "    #Remove corrections, if they have no correction (likely misclassified spelling mistake)\n",
    "    remove_no_correction = [x for x in corrections if x not in misspelled]\n",
    "    return remove_no_correction\n",
    "\n",
    "misspelled_words(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspellings_feature = []\n",
    "\n",
    "# for i in tqdm(range(len(text_pairs))):\n",
    "#     num_of_misspellings = len(misspelled_words(corpus[2*1])) - len(misspelled_words(corpus[2*i+1]))\n",
    "#     misspellings_feature.append(np.abs(num_of_misspellings))\n",
    "    \n",
    "# misspellings_feature = np.vstack(misspellings_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_features(feature_dict):\n",
    "    '''Save the updated feature dictionary. Takes dictionary as input and saves as binary file\n",
    "    \n",
    "    example: \n",
    "    >>> my_featues = {'freqdist': [1,6,3,5]}\n",
    "    >>> save_features(my_features)'''\n",
    "    \n",
    "    with open('data/features.dat', 'wb') as file:\n",
    "        pickle.dump(feature_dict, file)\n",
    "    print(\"Features saved! :-)\")\n",
    "\n",
    "def load_features():\n",
    "    '''Load feature dictionary. Returns the saved feature as a dictionary.\n",
    "    Will then print all the available features.\n",
    "    \n",
    "    example: \n",
    "    >>> my_features = load_features()'''\n",
    "    \n",
    "    with open('data/features.dat', 'rb') as file:\n",
    "        feats = pickle.load(file)\n",
    "    print(\"Features available:\")\n",
    "    for i in feats.keys():\n",
    "        print(i)\n",
    "    \n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_matrix = function words, profanity words, avg sentence length, avg word length, lix, yules i, number of misspellings\n",
    "feat_matrix = np.load(\"feature_matrix.dat\", allow_pickle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.92239211,  0.03173604,  0.57456802, ...,  3.93799665,\n",
       "        10.98592568, 16.        ],\n",
       "       [ 0.75270986,  0.19542507,  2.14250999, ...,  3.4817081 ,\n",
       "         2.18477903,  1.        ],\n",
       "       [ 0.92438127,  0.        ,  0.68910175, ...,  6.82064284,\n",
       "         0.57238363,  3.        ],\n",
       "       ...,\n",
       "       [ 0.92556229,  0.        ,  2.91769646, ...,  2.76283631,\n",
       "         0.45653242,  1.        ],\n",
       "       [ 0.88020293,  0.        ,  3.09249311, ...,  6.5335426 ,\n",
       "         2.48188354,  3.        ],\n",
       "       [ 0.7391699 ,  0.        ,  1.54155921, ...,  2.29202822,\n",
       "         0.79493386,  1.        ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_scalars = preprocessing.normalize(feat_matrix[:,2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02900049, 0.00148426, 0.1987647 , 0.55449876, 0.80757693],\n",
       "       [0.45075178, 0.0687992 , 0.73249886, 0.45964455, 0.21038491],\n",
       "       [0.09176661, 0.03414141, 0.90829441, 0.07622344, 0.39950534],\n",
       "       ...,\n",
       "       [0.70018754, 0.02330171, 0.66302427, 0.10955846, 0.23997957],\n",
       "       [0.37659038, 0.01869928, 0.79562644, 0.302233  , 0.36532697],\n",
       "       [0.50464247, 0.08649417, 0.75031486, 0.26022834, 0.32735847]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.hstack((feat_matrix[:,:2],normalized_scalars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_lix = feat_matrix[:,[0,1,2,3,5]]\n",
    "without_avg_word = feat_matrix[:,[0,1,2,4,5]]\n",
    "without_fw = feat_matrix[:,[1,2,3,4,5]]\n",
    "without_profanity = feat_matrix[:,[0,2,3,4,5]]\n",
    "without_avg_sent = feat_matrix[:,[0,1,3,4,5]]\n",
    "without_yules = feat_matrix[:,[0,1,2,3,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feat_matrix, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = svm_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6170886075949367"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1., -1., -1., ...,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.dual_coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_clf = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = random_forest_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.32397729, 0.04579973, 0.16396865, 0.15928598, 0.12239722,\n",
       "       0.11438042, 0.0701907 ])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6962025316455697"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, prediction) #random_state decreased the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on feature combinations: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When train/test split is 80/20\n",
    "\n",
    "Features = function words, profanity words, avg sentence length, avg word length, lix, yules i, number of misspellings difference\n",
    "\n",
    "\n",
    "**All:** \n",
    "    SVM acc = **0.617**0886075949367,\n",
    "    RF acc = **0.724**6835443037974 \n",
    "\n",
    "**Without number of misspellings difference:** \n",
    "    SVM acc = **0.648**7341772151899,\n",
    "    RF acc = **0.721**5189873417721\n",
    "    \n",
    "**Without lix:** \n",
    "    SVM acc = **0.655**0632911392406,\n",
    "    RF acc = **0.727**8481012658228\n",
    "    \n",
    "**Without yules I:** \n",
    "    SVM acc = **0.645**5696202531646,\n",
    "    RF acc = **0.712**0253164556962\n",
    "\n",
    "**Without average sentence length:**\n",
    "    SVM acc = **0.563**2911392405063,\n",
    "    RF acc = **0.718**3544303797469\n",
    "\n",
    "**Without average word length:**\n",
    "    SVM acc = **0.642**4050632911392,\n",
    "    RF acc = **0.674**0506329113924\n",
    "    \n",
    "**Without function words:** \n",
    "    SVM acc = **0.645**5696202531646,\n",
    "    RF acc = **0.648**7341772151899\n",
    "    \n",
    "**Without profanity words:** \n",
    "    SVM acc = **0.645**5696202531646,\n",
    "    RF acc = **0.699**3670886075949 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒'֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒֒m\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1426]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
