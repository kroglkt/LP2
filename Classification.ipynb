{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification notebook\n",
    "### No feature extraction here - will load from pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_emot():\n",
    "    e = [\"(o_o)\",\":-)\",\":P\",\":D\",\"x)\",\"ᓚᘏᗢ\",\"╯°□°）╯︵ ┻━┻\",\":)\",\n",
    "         \"*<:-)\",\"^_^\",\"(⌐■_■)\",\"¯\\_(ツ)_/¯\", \"(T_T)\",\":o\",\"OwO\",\n",
    "        \"( ͡❛ ͜ʖ ͡❛)\",\"(̶◉͛‿◉̶)\",\"( ≖.≖)\",\"(ㆆ_ㆆ)\",\"ʕ•́ᴥ•̀ʔっ\",\"( ◡́.◡̀)\",\"(^◡^ )\"]\n",
    "    return random.choice(e)\n",
    "\n",
    "def load_files():\n",
    "    text_pairs = [] #Would be nice to have as np.array\n",
    "    labels = []\n",
    "    fandom = []\n",
    "    \n",
    "    pair_id = []\n",
    "    true_id = []\n",
    "    \n",
    "    #Load truth JSON\n",
    "    for line in open('data/modified/train_truth.jsonl'):\n",
    "        d = json.loads(line.strip())\n",
    "        labels.append(int(d['same']))\n",
    "        true_id.append(d['id'])\n",
    "\n",
    "    #Load actual fanfic.\n",
    "    print(\"loading fanfic...\",rand_emot())\n",
    "    for line in tqdm(open('data/modified/train_pair.jsonl')):\n",
    "        d = json.loads(line.strip())\n",
    "        text_pairs.append(d['pair'])\n",
    "        fandom.append(d['fandoms'])\n",
    "        pair_id.append(d['id'])\n",
    "\n",
    "    print(\"done loading\",rand_emot())\n",
    "    \n",
    "    return text_pairs, labels, fandom, pair_id, true_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "307it [00:00, 3047.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading fanfic... (ㆆ_ㆆ)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15780it [00:05, 2978.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading (o_o)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, labels, _, _, _ = load_files()    #11550\n",
    "labels.pop(11550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features(feature_dict, filename='features'):\n",
    "    '''Save the updated feature dictionary. Takes dictionary as input and saves as binary file\n",
    "    \n",
    "    example: \n",
    "    >>> my_featues = {'freqdist': [1,6,3,5]}\n",
    "    >>> save_features(my_features)'''\n",
    "    \n",
    "    with open('data/{}.dat'.format(filename), 'wb') as file:\n",
    "        pickle.dump(feature_dict, file)\n",
    "    print(\"Features saved! :-)\")\n",
    "\n",
    "def load_features(filename='features'):\n",
    "    '''Load feature dictionary. Returns the saved feature as a dictionary.\n",
    "    \n",
    "    example: \n",
    "    >>> my_features = load_features()'''\n",
    "    \n",
    "    with open('data/{}.dat'.format(filename), 'rb') as file:\n",
    "        feats = pickle.load(file)\n",
    "    print(\"Features available:\")\n",
    "    for i in feats.keys():\n",
    "        print(i)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features available:\n",
      "character_bigram\n",
      "skip_bigram\n",
      "pos_skipgram\n",
      "pos_bigram\n",
      "character_bigram_cossim\n",
      "skip_bigram_cossim\n",
      "pos_skipgram_cossim\n",
      "pos_bigram_cossim\n",
      "function_word_dist\n",
      "profanity_dist\n",
      "unique_words\n",
      "punctuation_proportion\n",
      "avg_word_length\n",
      "avg_sent_length\n",
      "yules\n",
      "lix\n"
     ]
    }
   ],
   "source": [
    "feats = load_features(filename=\"big_features_yasmin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character_bigram \t (15779, 6000)\n",
      "skip_bigram \t (15779, 8000)\n",
      "pos_skipgram \t (15779, 2242)\n",
      "pos_bigram \t (15779, 6000)\n",
      "character_bigram_cossim \t (15779, 1)\n",
      "skip_bigram_cossim \t (15779, 1)\n",
      "pos_skipgram_cossim \t (15779, 1)\n",
      "pos_bigram_cossim \t (15779, 1)\n",
      "function_word_dist \t (15779, 1)\n",
      "profanity_dist \t (15779, 1)\n",
      "unique_words \t (15779, 1)\n",
      "punctuation_proportion \t (15779, 1)\n",
      "avg_word_length \t (15779, 1)\n",
      "avg_sent_length \t (15779, 1)\n",
      "yules \t (15779, 1)\n",
      "lix \t (15779, 1)\n"
     ]
    }
   ],
   "source": [
    "for i in feats:\n",
    "    if len(feats[i].shape)==1:\n",
    "        feats[i] = feats[i][:,None]\n",
    "    print(i, \"\\t\", feats[i].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strat_kfold(model, X, k):\n",
    "    y = np.array(labels)\n",
    "    targets = []\n",
    "    preds = []\n",
    "    preds_proba = []\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=69)\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        preds.append(model.predict(X_test))\n",
    "        preds_proba.append(model.predict_proba(X_test))\n",
    "        targets.append(y_test)\n",
    "    \n",
    "    return np.array([item for elem in preds for item in elem]), np.array([item for elem in targets for item in elem]), np.array([item for elem in preds_proba for item in elem])\n",
    "\n",
    "def combine_features(features):\n",
    "    #List of features as input\n",
    "    return np.hstack((features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features: (15779, 22254)\n",
      "Scalar features: (15779, 12)\n",
      "Forced scalar features: (15779, 16)\n",
      "Vector features: (15779, 16242)\n",
      "Final features: (15779, 12)\n"
     ]
    }
   ],
   "source": [
    "#Stratified 2-Fold accuracy on ALL Features\n",
    "all_features = combine_features([feats[x] for x in feats])\n",
    "scalar_features = combine_features([feats[x] for x in feats if feats[x].shape[1]==1])\n",
    "forced_scalars = []\n",
    "for i in feats:\n",
    "    if feats[i].shape[1] >1:\n",
    "        forced_scalars.append(np.mean(feats[i], axis=1)[:,None])\n",
    "    else:\n",
    "        forced_scalars.append(feats[i])\n",
    "\n",
    "forced_scalars = combine_features(forced_scalars)\n",
    "vector_features = combine_features([feats['character_bigram'],\n",
    "                                  feats['skip_bigram'],\n",
    "                                  feats['pos_skipgram']])\n",
    "\n",
    "final_features = combine_features([feats['function_word_dist'],\n",
    "                                 feats['profanity_dist'],\n",
    "                                 feats['avg_sent_length'],\n",
    "                                 feats['avg_word_length'],\n",
    "                                 feats['lix'],\n",
    "                                 feats['yules'],\n",
    "                                 feats['unique_words'],\n",
    "                                 feats['punctuation_proportion'],\n",
    "                                 feats['character_bigram_cossim'],\n",
    "                                 feats['skip_bigram_cossim'],\n",
    "                                 feats['pos_skipgram_cossim'],\n",
    "                                 feats['pos_bigram_cossim']])\n",
    "\n",
    "print(\"All features:\",all_features.shape)\n",
    "print(\"Scalar features:\", scalar_features.shape)\n",
    "print(\"Forced scalar features:\", forced_scalars.shape)\n",
    "print(\"Vector features:\", vector_features.shape)\n",
    "print(\"Final features:\", final_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_different_feats(model):\n",
    "    all_preds, all_targs, all_pred_proba = strat_kfold(model, all_features, 5)\n",
    "    vec_preds, vec_targs, vec_pred_proba = strat_kfold(model, vector_features, 5)\n",
    "    fin_preds, fin_targs, fin_pred_proba = strat_kfold(model, final_features, 5)\n",
    "    \n",
    "    print(\"All features: F1={}\\tAcc={}\".format(f1_score(all_targs, all_preds), accuracy_score(all_targs, all_preds)))\n",
    "    print(\"Vector features: F1={}\\tAcc={}\".format(f1_score(vec_targs, vec_preds), accuracy_score(vec_targs, vec_preds)))\n",
    "    print(\"Final features: F1={}\\tAcc={}\".format(f1_score(fin_targs, fin_preds), accuracy_score(fin_targs, fin_preds)))\n",
    "    \n",
    "    return [(all_pred_proba, all_targs), (vec_pred_proba, vec_targs), (fin_pred_proba, fin_targs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features: F1=0.8252336448598131\tAcc=0.8103808859877052\n",
      "Vector features: F1=0.7611021069692058\tAcc=0.7197541035553584\n",
      "Final features: F1=0.7999257379788354\tAcc=0.7951074212560999\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=500, random_state=42, n_jobs=4)\n",
    "rfc_results = try_different_feats(rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stratified 5-Fold on the 8k best Random Forest Features\n",
    "#rfc_8k_feats = all_features[:,rfc.feature_importances_.argsort()[-8000:][::-1]]\n",
    "#strat_kfold(rfc, rfc_8k_feats, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7987325728770596"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(final_features, labels)\n",
    "\n",
    "dist = {\n",
    "    'n_estimators' : [100,200,300,500,1000],\n",
    "    'min_samples_split': [2,4,6,8],\n",
    "    'min_samples_leaf': [1,2,3,4,5],\n",
    "}\n",
    "random_search = RandomizedSearchCV(rfc, dist, n_iter=20, random_state=42)\n",
    "search = random_search.fit(X_train, y_train)\n",
    "params = search.best_params_\n",
    "search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 1, ..., 0, 0, 1]),\n",
       " array([0, 0, 0, ..., 0, 0, 1]),\n",
       " array([[0.73931614, 0.26068386],\n",
       "        [0.83187963, 0.16812037],\n",
       "        [0.01561111, 0.98438889],\n",
       "        ...,\n",
       "        [0.88373137, 0.11626863],\n",
       "        [0.75604036, 0.24395964],\n",
       "        [0.2008957 , 0.7991043 ]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strat_kfold(search, final_features, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:38:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:49:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:00:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:11:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:21:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:32:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:41:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:49:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:58:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:06:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:14:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:14:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:14:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:14:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:14:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "All features: F1=0.8405637254901961\tAcc=0.835097281196527\n",
      "Vector features: F1=0.7961746068997887\tAcc=0.7798339565244946\n",
      "Final features: F1=0.7905196889272929\tAcc=0.7849039863109196\n"
     ]
    }
   ],
   "source": [
    "xgb = xgboost.XGBClassifier(random_state=42)\n",
    "xgb_results = try_different_feats(xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7934093789607097"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(final_features, labels, random_state=42)\n",
    "\n",
    "dist = {\n",
    "    'n_estimators' : [100,200,300,500,1000,2000],\n",
    "    'max_depth' : [6,7,8,9,10],\n",
    "    'gamma' : [0,1,2,3,5,8],\n",
    "    'learning_rate':[0.003, 0.01, 0.03, 0.1],\n",
    "    \"eval_metric\": [\"logloss\", \"error\"],\n",
    "}\n",
    "random_search = RandomizedSearchCV(xgb, dist, n_iter=20, random_state=42)\n",
    "search = random_search.fit(X_train, y_train)\n",
    "params = search.best_params_\n",
    "search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = svm.SVC(probability=True, random_state=42)\n",
    "svc_results = try_different_feats(svm_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(final_features, labels)\n",
    "\n",
    "dist = {\n",
    "    'C' : [1,2,3],\n",
    "    'gamma' : ['scale', 'auto'],\n",
    "}\n",
    "random_search = RandomizedSearchCV(svm_clf, dist, n_iter=20, random_state=42)\n",
    "search = random_search.fit(X_train, y_train)\n",
    "params = search.best_params_\n",
    "search.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrg = LogisticRegression(max_iter=2000, n_jobs=4, random_state=42)\n",
    "lrg_results = try_different_feats(lrg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB(random_state=42)\n",
    "nb_results = try_different_feats(nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter=1000, random_state=42)\n",
    "mlp_results = try_different_feats(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(random_state=42)\n",
    "dummy_results = try_different_feats(dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_cur(fper, tper, label=\"\", title=\"plot title\"):\n",
    "    plt.plot(fper, tper, label=label)\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "def create_roc_curve(preds, targets, label=\"\", title=\"\"):\n",
    "    all_fpr, all_tpr = [], []\n",
    "    print(preds[:,1])\n",
    "    fpr, tpr, thresholds = roc_curve(targets, preds[:,1]) #De her burde vist være nyttet om...\n",
    "    all_fpr.append(fpr)\n",
    "    all_tpr.append(tpr)\n",
    "    all_fpr = np.array(all_fpr)\n",
    "    all_tpr = np.array(tpr)\n",
    "    \n",
    "    plot_roc_cur(all_fpr[0], all_tpr, label=label, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Random Forest', 'Support Vector Machine', 'Logistic Regression', 'XGBoost', 'Multilayer Perceptron']\n",
    "all_results = [rfc_results[0][0], svc_results[0][0], lrg_results[0][0], xgb_results[0][0], mlp_results[0][0]]\n",
    "all_targets = [rfc_results[0][1], svc_results[0][1], lrg_results[0][1], xgb_results[0][1], mlp_results[0][1]]\n",
    "\n",
    "vec_results = [rfc_results[1][0], svc_results[1][0], lrg_results[1][0], xgb_results[1][0], mlp_results[1][0]]\n",
    "vec_targets = [rfc_results[1][1], svc_results[1][1], lrg_results[1][1], xgb_results[1][1], mlp_results[1][1]]\n",
    "\n",
    "fin_results = [rfc_results[2][0], svc_results[2][0], lrg_results[2][0], xgb_results[2][0], mlp_results[2][0]]\n",
    "fin_targets = [rfc_results[2][1], svc_results[2][1], lrg_results[2][1], xgb_results[2][1], mlp_results[2][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_results)):\n",
    "    create_roc_curve(all_results[i], all_targets[i], label=labels[i], title=\"ROC Curve For All Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(vec_results)):\n",
    "    create_roc_curve(vec_results[i], vec_targets[i], label=labels[i], title=\"ROC Curve For Vector Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fin_results)):\n",
    "    create_roc_curve(fin_results[i], fin_targets[i], label=labels[i], title=\"ROC Curve For Scalar Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
