{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I guess that for doc2vec it is not wise to use text pairs, because it will better predict the author of the text we fit into it rather than just 1/0 value of whether the text is written by the same author or not.\n",
    "On the other hand, doc2vec is a way to vectorize documents, and I think we can try to use it instead of tf-idf.\n",
    "\n",
    "Also found an example f how to use doc2vec with log regression: https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c anaconda gensim\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I like them :)\n",
    "\n",
    "def rand_emot():\n",
    "    e = [\"(o_o)\",\":-)\",\":P\",\":D\",\"x)\",\"ᓚᘏᗢ\",\"╯°□°）╯︵ ┻━┻\",\":)\",\n",
    "         \"*<:-)\",\"^_^\",\"(⌐■_■)\",\"¯\\_(ツ)_/¯\", \"(T_T)\",\":o\",\"OwO\",\n",
    "        \"( ͡❛ ͜ʖ ͡❛)\",\"(̶◉͛‿◉̶)\",\"( ≖.≖)\",\"(ㆆ_ㆆ)\",\"ʕ•́ᴥ•̀ʔっ\",\"( ◡́.◡̀)\",\"(^◡^ )\"]\n",
    "    return random.choice(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nblablabla= [\\n   'когда всё это закончится',\\n   'плачет киска в коридоре у неё большое горе',\\n   'когда теряет равновесие твоё сознание усталое',\\n   'трактор велосипед дом стена бумажка кресло',\\n   'почему всё не так вроде всё как всегда'\\n]\\n \\ndocuments = [TaggedDocument(doc, [i]) for i, doc in enumerate(some_texts)]\\n \\nmodel = Doc2Vec(documents, vector_size = 5, workers = 4, epochs = 3)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just test the model on the tiny sample\n",
    "\n",
    "\"\"\"\n",
    "blablabla= [\n",
    "   'когда всё это закончится',\n",
    "   'плачет киска в коридоре у неё большое горе',\n",
    "   'когда теряет равновесие твоё сознание усталое',\n",
    "   'трактор велосипед дом стена бумажка кресло',\n",
    "   'почему всё не так вроде всё как всегда'\n",
    "]\n",
    " \n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(some_texts)]\n",
    " \n",
    "model = Doc2Vec(documents, vector_size = 5, workers = 4, epochs = 3)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_emot():\n",
    "    e = [\"(o_o)\",\":-)\",\":P\",\":D\",\"x)\",\"ᓚᘏᗢ\",\"╯°□°）╯︵ ┻━┻\",\":)\",\n",
    "         \"*<:-)\",\"^_^\",\"(⌐■_■)\",\"¯\\_(ツ)_/¯\", \"(T_T)\",\":o\",\"OwO\",\n",
    "        \"( ͡❛ ͜ʖ ͡❛)\",\"(̶◉͛‿◉̶)\",\"( ≖.≖)\",\"(ㆆ_ㆆ)\",\"ʕ•́ᴥ•̀ʔっ\",\"( ◡́.◡̀)\",\"(^◡^ )\"]\n",
    "    return random.choice(e)\n",
    "\n",
    "def load_files():\n",
    "    text_pairs = [] #Would be nice to have as np.array\n",
    "    labels = []\n",
    "    fandom = []\n",
    "    \n",
    "    pair_id = []\n",
    "    true_id = []\n",
    "    author = []\n",
    "    \n",
    "    #Load truth JSON\n",
    "    for line in open('/Users/olgaiarygina/Downloads/LP2/data/modified/train_truth.jsonl'):\n",
    "        d = json.loads(line.strip())\n",
    "        labels.append(int(d['same']))\n",
    "        true_id.append(d['id'])\n",
    "        author.append(d['authors'])\n",
    "        \n",
    "\n",
    "    #Load actual fanfic.\n",
    "    print(\"loading fanfic...\",rand_emot())\n",
    "    for line in tqdm(open('/Users/olgaiarygina/Downloads/LP2/data/modified/train_pair.jsonl')):\n",
    "        d = json.loads(line.strip())\n",
    "        text_pairs.append(d['pair'])\n",
    "        fandom.append(d['fandoms'])\n",
    "        pair_id.append(d['id'])\n",
    "\n",
    "\n",
    "    print(\"done loading\",rand_emot())\n",
    "    \n",
    "    return text_pairs, labels, fandom, pair_id, true_id, author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "161it [00:00, 1604.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading fanfic... ╯°□°）╯︵ ┻━┻\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1578it [00:00, 2543.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading :-)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_pairs, labels, fandom, pair_id, true_id, author = load_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in text_pairs for item in sublist]\n",
    "flat_author = [item for sublist in author for item in sublist]\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(flat_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nlittle_doc_test = documents[:15]\\nlittle_author_test = author[:15]\\nmodel = Doc2Vec(little_doc_test, vector_size = 5, workers = 4, epochs = 3) # epochs and vector_size are chosen intuitively\\n\\n# fit Random Forest\\nclf = RandomForestClassifier()\\nclf.fit([model.infer_vector([x.words]) for x in little_doc_test], little_author_test)\\n  \\nres = clf.predict([model.infer_vector(['text', 'Harry', 'irrestible'])]) # don't ask me why these words\\nprint(res) \\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test on a very small dataset\n",
    "\n",
    "\"\"\"\n",
    "little_doc_test = documents[:15]\n",
    "little_author_test = author[:15]\n",
    "model = Doc2Vec(little_doc_test, vector_size = 5, workers = 4, epochs = 3) # epochs and vector_size are chosen intuitively\n",
    "\n",
    "# fit Random Forest\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit([model.infer_vector([x.words]) for x in little_doc_test], little_author_test)\n",
    "  \n",
    "res = clf.predict([model.infer_vector(['text', 'Harry', 'irrestible'])]) # don't ask me why these words\n",
    "print(res) \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes a while!\n",
    "\n",
    "#model.build_vocab(documents)\n",
    "model = Doc2Vec(documents, vector_size = 100, workers = 4, epochs = 15) # epochs and vector_size are chosen intuitively following some instructions from stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.doc2vec.Doc2Vec at 0x7fa612d2f520>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvector_to_search = model.infer_vector([\"find\", \"similar\", \"text\"])\\n\\n# три наиболее похожих\\n# 3 most similar\\nsimilar_documents = model.docvecs.most_similar([vector_to_search], topn = 3)\\nfor s in similar_documents:\\n    print(flat_list[s[0]])\\n    '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# нахождение наиболее похожего документа\n",
    "# finding the most similar docs\n",
    "\n",
    "\"\"\"\n",
    "vector_to_search = model.infer_vector([\"find\", \"similar\", \"text\"])\n",
    "\n",
    "# три наиболее похожих\n",
    "# 3 most similar\n",
    "similar_documents = model.docvecs.most_similar([vector_to_search], topn = 3)\n",
    "for s in similar_documents:\n",
    "    print(flat_list[s[0]])\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit Random Forest\n",
    "# for the whole data takes a while as well\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit([model.infer_vector([x.words]) for x in documents], flat_author)\n",
    "  \n",
    "res = clf.predict([model.infer_vector(['text', 'Harry', 'irrestible'])]) # don't ask me why these words\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
