{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_emot():\n",
    "    e = [\"(o_o)\",\":-)\",\":P\",\":D\",\"x)\",\"ᓚᘏᗢ\",\"╯°□°）╯︵ ┻━┻\",\":)\",\n",
    "         \"*<:-)\",\"^_^\",\"(⌐■_■)\",\"¯\\_(ツ)_/¯\", \"(T_T)\",\":o\",\"OwO\",\n",
    "        \"( ͡❛ ͜ʖ ͡❛)\",\"(̶◉͛‿◉̶)\",\"( ≖.≖)\",\"(ㆆ_ㆆ)\",\"ʕ•́ᴥ•̀ʔっ\",\"( ◡́.◡̀)\",\"(^◡^ )\"]\n",
    "    return random.choice(e)\n",
    "\n",
    "def load_files():\n",
    "    text_pairs = [] #Would be nice to have as np.array\n",
    "    labels = []\n",
    "    fandom = []\n",
    "    \n",
    "    pair_id = []\n",
    "    true_id = []\n",
    "    \n",
    "    #Load truth JSON\n",
    "    for line in open('data/modified/train_truth.jsonl'):\n",
    "        d = json.loads(line.strip())\n",
    "        labels.append(int(d['same']))\n",
    "        true_id.append(d['id'])\n",
    "\n",
    "    #Load actual fanfic.\n",
    "    print(\"loading fanfic...\",rand_emot())\n",
    "    for line in tqdm(open('data/modified/train_pair.jsonl')):\n",
    "        d = json.loads(line.strip())\n",
    "        text_pairs.append(d['pair'])\n",
    "        fandom.append(d['fandoms'])\n",
    "        pair_id.append(d['id'])\n",
    "\n",
    "    print(\"done loading\",rand_emot())\n",
    "    \n",
    "    return text_pairs, labels, fandom, pair_id, true_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "140it [00:00, 1386.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading fanfic... ( ≖.≖)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1578it [00:00, 1762.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading OwO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_pairs, labels, fandom, pair_id, true_id = load_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word frequency and word frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_distribution(text_pair): #expect untokenized input\n",
    "    \n",
    "    pair = []\n",
    "    \n",
    "    for text in text_pair: \n",
    "        tokens = nltk.word_tokenize(text) #tokenize\n",
    "        \n",
    "        freq_dist = nltk.FreqDist(tokens) #compute frequency distribution\n",
    "        pair.append(freq_dist)\n",
    "        \n",
    "    return pair #return frequency distribution of each fanfic in the input pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq(text_pair): #expects tokenized pairs\n",
    "    fdist0 = nltk.FreqDist(text_pair[0])\n",
    "    fdist1 = nltk.FreqDist(text_pair[1])\n",
    "    \n",
    "    return [fdist0, fdist1]\n",
    "\n",
    "def word_freq_single(text):\n",
    "    fdist = nltk.FreqDist(text)\n",
    "    return fdist\n",
    "\n",
    "def tokenize(text_pair):\n",
    "    return [nltk.word_tokenize(text_pair[0]),nltk.word_tokenize(text_pair[1])]\n",
    "\n",
    "def vector_freq_dist(freq_dists): #I don't think this works...\n",
    "    return [list(freq_dists[0].values()), list(freq_dists[1].values())]\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)+0.0000000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(text_pairs):\n",
    "    '''input all text pairs to create a corpus'''\n",
    "    corpus = [x[i] for x in text_pairs for i in range(len(x))]\n",
    "    return corpus\n",
    "\n",
    "def fit_tfidf(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    print(\"training vectorizer...\",rand_emot())\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    print(\"vectorizer fit!\", rand_emot())\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(X[0].T.todense(), index=vectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "    df = df.sort_values('TF-IDF', ascending=False)\n",
    "    \n",
    "    return X, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = create_corpus(text_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training vectorizer... ^_^\n",
      "vectorizer fit! (^◡^ )\n"
     ]
    }
   ],
   "source": [
    "#tf-idf on the raw text. Likely not useful, as you can see, it is sesnitive to the fandom.\n",
    "raw_tfidf, tfidf_df = fit_tfidf(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_tfidf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4811 1176\n",
      "4893 1414\n",
      "5010 1018\n",
      "4785 1324\n",
      "4600 1169\n",
      "4912 1073\n",
      "5094 1076\n",
      "4995 1092\n",
      "4946 1099\n",
      "4938 1170\n",
      "5048 1045\n",
      "5178 1059\n",
      "5131 1084\n",
      "4650 950\n",
      "5052 1277\n",
      "4974 1092\n",
      "4912 970\n",
      "5333 942\n",
      "4916 1016\n",
      "4845 1019\n",
      "5385 1024\n",
      "4820 1196\n",
      "5007 908\n",
      "5102 990\n",
      "5060 943\n",
      "4631 1397\n",
      "4856 1103\n",
      "4620 1254\n",
      "5027 1056\n",
      "5081 1106\n",
      "4891 1157\n",
      "5269 1026\n",
      "5267 1144\n",
      "5076 1133\n",
      "4948 983\n",
      "5351 1148\n",
      "4857 1136\n",
      "4791 1087\n",
      "4351 1359\n",
      "4494 1278\n",
      "4882 1226\n",
      "6175 1303\n",
      "4711 1060\n",
      "4776 1151\n",
      "5352 1010\n",
      "5460 1156\n",
      "4903 1115\n",
      "5209 1190\n",
      "5338 1098\n",
      "5101 1129\n",
      "4971 1121\n",
      "5067 859\n",
      "5159 1020\n",
      "5188 1004\n",
      "4780 1024\n",
      "4562 1213\n",
      "5010 1186\n",
      "4884 1119\n",
      "5100 1127\n",
      "4795 1062\n",
      "5210 1068\n",
      "4628 1127\n",
      "4894 1192\n",
      "4832 1080\n",
      "5170 712\n",
      "5153 702\n",
      "5453 888\n",
      "4605 1000\n",
      "5007 1174\n",
      "4860 1294\n",
      "5106 1187\n",
      "4875 1086\n",
      "4767 1098\n",
      "4779 1205\n",
      "4780 1085\n",
      "4477 1128\n",
      "4847 974\n",
      "4709 1060\n",
      "5421 879\n",
      "5322 824\n",
      "4862 1165\n",
      "4837 1034\n",
      "5223 1016\n",
      "4803 1212\n",
      "4981 966\n",
      "5314 986\n",
      "5339 944\n",
      "4886 1271\n",
      "5040 1264\n",
      "4960 979\n",
      "4709 1046\n",
      "5132 1208\n",
      "4877 1173\n",
      "4669 1278\n",
      "5028 1055\n",
      "5107 997\n",
      "4804 1321\n",
      "4973 1145\n",
      "4938 1156\n",
      "5396 1307\n",
      "5055 1097\n",
      "4871 1197\n",
      "4878 1105\n",
      "5275 958\n",
      "4822 975\n",
      "5030 893\n",
      "4890 1088\n",
      "5026 1127\n",
      "5283 988\n",
      "5313 993\n",
      "5158 949\n",
      "5201 1006\n",
      "4884 1110\n",
      "4941 1059\n",
      "4559 1128\n",
      "4607 1041\n",
      "5014 1040\n",
      "5122 988\n",
      "5297 1057\n",
      "4886 1188\n",
      "5148 1095\n",
      "4946 1079\n",
      "5213 882\n",
      "4860 1138\n",
      "5426 939\n",
      "5322 1009\n",
      "4750 1170\n",
      "4858 1248\n",
      "4913 1408\n",
      "4914 1282\n",
      "4695 915\n",
      "5140 1211\n",
      "5584 793\n",
      "4584 1077\n",
      "4941 1119\n",
      "4731 1114\n",
      "4718 1087\n",
      "5006 1188\n",
      "4784 945\n",
      "4837 1028\n",
      "4888 1244\n",
      "5156 1217\n",
      "5299 1136\n",
      "4934 1139\n",
      "4515 1040\n",
      "4787 995\n",
      "4464 1130\n",
      "5096 1176\n",
      "4812 1147\n",
      "4791 1234\n",
      "4783 1060\n",
      "5264 1011\n",
      "4820 1244\n",
      "5005 973\n",
      "5473 1319\n",
      "4896 1212\n",
      "4908 990\n",
      "4675 958\n",
      "5169 987\n",
      "4845 1284\n",
      "4870 753\n",
      "4774 692\n",
      "5005 1238\n",
      "4660 1286\n",
      "5089 959\n",
      "4857 1079\n",
      "4732 1063\n",
      "5010 1109\n",
      "5205 976\n",
      "4457 1380\n",
      "5016 1152\n",
      "4685 1023\n",
      "4991 1055\n",
      "4847 1046\n",
      "4780 1262\n",
      "4819 1173\n",
      "4671 1241\n",
      "5011 1139\n",
      "4939 951\n",
      "5005 856\n",
      "4746 1183\n",
      "5006 1064\n",
      "4746 1061\n",
      "4578 1124\n",
      "4976 1085\n",
      "4580 1061\n",
      "4595 1249\n",
      "4814 1181\n",
      "4862 1034\n",
      "5025 1184\n",
      "5020 1051\n",
      "4981 1048\n",
      "5405 1088\n",
      "4871 1160\n",
      "5125 954\n",
      "4954 1151\n",
      "5371 980\n",
      "5043 1181\n",
      "5215 1248\n",
      "4640 1092\n",
      "4863 986\n",
      "5003 1199\n",
      "4942 787\n",
      "4675 1133\n",
      "4617 1168\n",
      "5062 1126\n",
      "5130 1096\n",
      "4873 1141\n",
      "4894 1060\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-f0d328fcc238>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0munique_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m    129\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     return [\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m     ]\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     return [\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m     ]\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\destructive.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;31m# Handles parentheses.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mregexp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPARENS_BRACKETS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[1;31m# Optionally convert parentheses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconvert_parentheses\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36m_subx\u001b[1;34m(pattern, template)\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;31m# internal: Pattern.sub/subn implementation helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[0mtemplate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_compile_repl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m         \u001b[1;31m# literal replacement\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def unique_words(text_pairs):\n",
    "    \n",
    "    richness_all = []\n",
    "    \n",
    "    for pair in text_pairs:\n",
    "        richness_pair = []\n",
    "        \n",
    "        for text in pair: \n",
    "            tokens = nltk.word_tokenize(text)\n",
    "            unique_tokens = list(set(tokens))\n",
    "            richness = unique_tokens / tokens\n",
    "        \n",
    "            richness_pair.append(richness)\n",
    "            \n",
    "        richness_all.append(richness_pair)\n",
    "    \n",
    "    richness_feat = []\n",
    "    \n",
    "    for i in range(len(text_pairs)):\n",
    "        rich_feat = \n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempting to perform tf-idf on only symbols.\n",
    "def isolate_symbols(corpus):\n",
    "    #Add \\d to omit digits too.\n",
    "    sym_corpus = []\n",
    "    for text in corpus:\n",
    "        sym_corpus.append(' '.join(re.findall(\"[^a-zA-Z\\s]+\", text)))\n",
    "    return sym_corpus\n",
    "\n",
    "symbols = isolate_symbols(corpus)\n",
    "\n",
    "#Okay, tf-idf doesn't work with symbols. I'll convert them to made-up words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training vectorizer... :D\n",
      "vectorizer fit! *<:-)\n"
     ]
    }
   ],
   "source": [
    "punct_matrix, punct_DF = fit_tfidf(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punct_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. punctuation divided by total no. tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make this a little better - fewer lines\n",
    "\n",
    "def punctuation_proportion(text_pairs, corpus, punctuation_corpus):\n",
    "    feature = []\n",
    "    \n",
    "    punc_prop_all = [] #punctuation proportion for all pairs \n",
    "    \n",
    "    for i in range(len(text_pairs)): \n",
    "        punc_prop_pair = [] #punctuation proportion for each pair\n",
    "        \n",
    "        punc_prop1 = len(symbols[2*i]) / len(corpus[2*i])\n",
    "        punc_prop2 = len(symbols[2*i+1]) / len(corpus[2*i+1])\n",
    "        punc_prop_pair.append(punc_prop1)\n",
    "        punc_prop_pair.append(punc_prop2)\n",
    "        punc_prop_all.append(punc_prop_pair)\n",
    "    \n",
    "    for i in range(len(text_pairs)):\n",
    "        feature_pair = punc_prop_all[i][0] - punc_prop_all[i][1]\n",
    "        feature.append(feature_pair)\n",
    "        \n",
    "    feature = np.vstack(np.abs(feature))\n",
    "        \n",
    "    return feature\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_proportion_feature = punctuation_proportion(text_pairs, corpus, symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS-tagging and Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## POS Tagging and ngrams\n",
    "tokens = nltk.word_tokenize(corpus[0])\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "pos_bigrams = nltk.bigrams(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIX calculation - LIX = readability index, a measure for the readability of a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lix(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    splt = text.split()\n",
    "    o = len(splt)+1\n",
    "    p = len([x for x in tokens if x=='.'])+1\n",
    "    l = len([x for x in tokens if len(x)>6])+1\n",
    "    \n",
    "    return (o/p)+((l*100)/o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text in corpus[:10]:\n",
    "#     print(compute_lix(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lix_feature = []\n",
    "\n",
    "# for i in tqdm(range(len(text_pairs))):\n",
    "#     lix = compute_lix(corpus[2*1]) - compute_lix(corpus[2*i+1])\n",
    "#     lix_feature.append(np.abs(lix))\n",
    "    \n",
    "# lix_feature = np.vstack(lix_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lix_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence and word length - compute sentence and word length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols(text):\n",
    "    sentences = re.split('[\\.+|!|?]', text)\n",
    "    sentences = [re.sub(r\"[^\\w]+\", ' ', x) for x in sentences if len(x.strip()) != 0]\n",
    "    return ' '.join(sentences)\n",
    "\n",
    "def get_sent_word_length(text):\n",
    "    #Function, which removes symbols and count words in sentence\n",
    "    #Output: length of each sentence & length of each word\n",
    "    sentences = re.split('[\\.+|!|?]', text)\n",
    "    sentences = [re.sub(r\"[^\\w]+\", ' ', x) for x in sentences if len(x.strip()) != 0]\n",
    "    word_sentences = [nltk.word_tokenize(x) for x in sentences]\n",
    "    sentence_lengths = np.array([len(x) for x in word_sentences])\n",
    "    word_lengths = np.array([len(s) for x in word_sentences for s in x])\n",
    "    return sentence_lengths, word_lengths\n",
    "\n",
    "# get_sent_word_length(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sent_len = []\n",
    "avg_word_len = []\n",
    "\n",
    "# for i in tqdm(range(len(text_pairs))):\n",
    "#     sent_length1, word_lengths1 = get_sent_word_length(corpus[i*2])\n",
    "#     sent_length2, word_lengths2 = get_sent_word_length(corpus[i*2+1])\n",
    "    \n",
    "#     avg_sent = np.average(sent_length1) - np.average(sent_length2)\n",
    "#     avg_word = np.average(word_lengths1) - np.average(word_lengths2)\n",
    "    \n",
    "#     avg_sent_len.append(np.abs(avg_sent))\n",
    "#     avg_word_len.append(np.abs(avg_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_sent_len = np.vstack(avg_sent_len)\n",
    "# avg_word_len = np.vstack(avg_word_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_sent_word_feat = np.hstack((avg_sent_len, avg_word_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolating function words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'data/modified/train_pair.jsonl'\n",
    "\n",
    "with open('data/function_words_clean.txt', \"r\") as fw:\n",
    "    func_words = fw.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def isolate_fw(data, f_words): #data must be json file - input must be path to file \n",
    "#     fw_in_data = []\n",
    "\n",
    "#     for line in tqdm(open(data)):\n",
    "#         function_words = []\n",
    "#         d = json.loads(line.strip()) #load the json file\n",
    "#         text = d.get(\"pair\") #get the actual fanfic\n",
    "#         words = text[0].split() #split fanfic into words in list\n",
    "#         for word in words: \n",
    "#             if word in f_words: #if the word is a function word\n",
    "#                 function_words.append(word)\n",
    "                \n",
    "#         stringed_function_words = \" \".join(function_words)\n",
    "        \n",
    "#         #append all function words as one long string in a list\n",
    "#         fw_in_data.append([stringed_function_words]) #fw_in_data is a list with lists\n",
    "#         #each list contains a string of all function words for each pair\n",
    "#         #should it be a string for each pair?\n",
    "        \n",
    "#     return fw_in_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = isolate_fw(data, func_words) #why is this so fast?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function isolates function words and return the feature (cosine similarity between texts in pair)\n",
    "# along with the isolated functions words (for the function word proportion feature\n",
    "\n",
    "def isolate_function_words(text_pairs, f_words): #data must be the text_pairs from load_files()\n",
    "    fw_in_data = []\n",
    "    \n",
    "    fw_text_pairs = []\n",
    "    for pair in tqdm(text_pairs):\n",
    "        fw_text_pairs = []\n",
    "        for text in pair: \n",
    "            function_words = []\n",
    "            \n",
    "            words = text.split() #split fanfic into words in list\n",
    "            \n",
    "            for word in words: \n",
    "                if word in f_words: #if the word is a function word\n",
    "                    function_words.append(word)\n",
    "                \n",
    "            stringed_function_words = \" \".join(function_words) #for each fanfic in a pair, makes FW a long string. \n",
    "            fw_text_pairs.append(stringed_function_words) \n",
    "            \n",
    "        #append text pairs with only their function words\n",
    "        fw_in_data.append(fw_text_pairs) \n",
    "        \n",
    "    fw_corpus = create_corpus(fw_in_data)  \n",
    "    FW_matrix, fw_dateframe = fit_tfidf(fw_corpus) #vectorize\n",
    "    fw_matrix = FW_matrix.toarray()\n",
    "    \n",
    "    feature = []\n",
    "\n",
    "    for i in range(len(text_pairs)):\n",
    "        cos_sim = cosine_sim(fw_matrix[2*i], fw_matrix[2*i+1]) #compute similarity \n",
    "        feature.append(cos_sim)\n",
    "        \n",
    "    feature = np.vstack(feature) #final feature stack\n",
    "    \n",
    "        \n",
    "    return feature, fw_in_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1578/1578 [00:56<00:00, 27.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training vectorizer... :P\n",
      "vectorizer fit! :P\n"
     ]
    }
   ],
   "source": [
    "function_words_feature, fw_in_data = isolate_function_words(text_pairs, func_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92239211],\n",
       "       [0.75270986],\n",
       "       [0.92438127],\n",
       "       ...,\n",
       "       [0.92556229],\n",
       "       [0.88020293],\n",
       "       [0.7391699 ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_words_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function words divided by total no. tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_words_proportion(text_pair_original, text_pair_fw): \n",
    "    fw_by_total = []\n",
    "    \n",
    "    for i, pair in enumerate(text_pair_original): \n",
    "        fw_by_total_pair = []\n",
    "        for ind, text in enumerate(pair): \n",
    "            fw_length = len(text_pair_fw[i][ind]) #length of text counting only function words\n",
    "            orig_length = len(text_pair_original[i][ind]) #length of text counting all tokens\n",
    "            proportion = fw_length/orig_length #divide occurance of function words by all tokens\n",
    "            fw_by_total_pair.append(proportion)\n",
    "        fw_by_total.append(fw_by_total_pair)\n",
    "        \n",
    "        \n",
    "    feature = []\n",
    "\n",
    "    for pair in fw_by_total: \n",
    "        feat = pair[0] - pair[1]\n",
    "        feature.append(np.abs(feat))\n",
    "\n",
    "    feature = np.vstack(feature)\n",
    "    \n",
    "    \n",
    "    return feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw_proportion_feature = function_words_proportion(text_pairs, fw_in_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04599114],\n",
       "       [0.01511811],\n",
       "       [0.01430844],\n",
       "       ...,\n",
       "       [0.04880898],\n",
       "       [0.08990624],\n",
       "       [0.00329041]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fw_proportion_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. unique words divided by total no. words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(text_pairs[:10])):\n",
    "    print(len(pair[i]),len(pair[i+1]))\n",
    "    tokens = tokenize(text_pairs[i])\n",
    "#     for text in tokens: \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolating profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'data/modified/train_pair.jsonl'\n",
    "with open('data/profanity_words_clean.txt', \"r\") as pr:\n",
    "    prof_words = pr.read().split()\n",
    "del prof_words[:4]\n",
    "del prof_words[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'data/modified/train_pair.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_profanity(data, prof_words):\n",
    "    profanity = []\n",
    "    \n",
    "    for pair in tqdm(data):\n",
    "        profanity_pairs = []\n",
    "        for text in pair:\n",
    "            resultwords = []\n",
    "\n",
    "            #d = json.loads(line.strip())\n",
    "            #text = d.get(\"pair\") \n",
    "            words = text.split() \n",
    "\n",
    "            resultwords  = [word for word in words if word.lower() in prof_words]\n",
    "\n",
    "            result = \" \".join(resultwords)\n",
    "            profanity_pairs.append(result)\n",
    "        \n",
    "        profanity.append(profanity_pairs) \n",
    "\n",
    "        \n",
    "    return profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profanity_inData = isolate_profanity(text_pairs, prof_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profanity_corpus = create_corpus(profanity_inData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profanity_matrix, profanity_dataframe = fit_tfidf(profanity_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profanity_matrix = profanity_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profanity_feature = []\n",
    "\n",
    "# for i in range(len(text_pairs)):\n",
    "#     cos_sim = cosine_sim(profanity_matrix[2*i], profanity_matrix[2*i+1])\n",
    "#     profanity_feature.append(cos_sim)\n",
    "# profanity_feature = np.vstack(profanity_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = np.hstack((fw_feature, profanity_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yule's K computations - different implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Our own implementation - delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_no_symbols(text):\n",
    "    return nltk.word_tokenize(re.sub(r'[^\\w]', ' ', text))\n",
    "\n",
    "def get_fdist_yule(text):\n",
    "    text = tokenize_no_symbols(text)\n",
    "    fdist = word_freq_single(text)\n",
    "    return fdist\n",
    "        \n",
    "def get_num_unique_words(text):\n",
    "    text = tokenize_no_symbols(text.lower())\n",
    "    return len(set(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Implementation below from:\n",
    "https://gist.github.com/magnusnissel/d9521cb78b9ae0b2c7d6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "\n",
    "def tokenize(s):\n",
    "    tokens = re.split(r\"[^0-9A-Za-z\\-'_]+\", s)\n",
    "    return tokens\n",
    "\n",
    "def get_yules(s):\n",
    "    \"\"\" \n",
    "    Returns a tuple with Yule's K and Yule's I.\n",
    "    (cf. Oakes, M.P. 1998. Statistics for Corpus Linguistics.\n",
    "    International Journal of Applied Linguistics, Vol 10 Issue 2)\n",
    "    In production this needs exception handling.\n",
    "    \"\"\"\n",
    "    tokens = tokenize(s)\n",
    "    token_counter = collections.Counter(tok.upper() for tok in tokens)\n",
    "    m1 = sum(token_counter.values())\n",
    "    m2 = sum([freq ** 2 for freq in token_counter.values()])\n",
    "    i = (m1*m1) / (m2-m1)\n",
    "    k = 1/i * 10000\n",
    "    return (k, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text in corpus[:10]: \n",
    "#     k_i = get_yules(text)\n",
    "#     print(k_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Implementation below from: https://swizec.com/blog/measuring-vocabulary-richness-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from itertools import groupby\n",
    "\n",
    "def words(entry):\n",
    "    return filter(lambda w: len(w) > 0,\n",
    "                  [w.strip(\"0123456789!:,.?(){}[]\") for w in entry.split()])\n",
    "\n",
    "def yule(entry):\n",
    "    # yule's I measure (the inverse of yule's K measure)\n",
    "    # higher number is higher diversity - richer vocabulary\n",
    "    d = {}\n",
    "    stemmer = PorterStemmer()\n",
    "    for w in words(entry):\n",
    "        w = stemmer.stem(w).lower()\n",
    "        try:\n",
    "            d[w] += 1\n",
    "        except KeyError:\n",
    "            d[w] = 1\n",
    "\n",
    "    M1 = float(len(d))\n",
    "    M2 = sum([len(list(g))*(freq**2) for freq,g in groupby(sorted(d.values()))])\n",
    "\n",
    "    try:\n",
    "        return (M1*M1)/(M2-M1)\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text in corpus[:10]: \n",
    "#     yules_i = yule(text) #yules_i = inverse of yules K  - why do you want the inverse instead? \n",
    "#     print(yules_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yules_i_feature = []\n",
    "\n",
    "# for i in tqdm(range(len(text_pairs))):\n",
    "#     yules_i = yule(corpus[2*i]) - yule(corpus[2*i+1]) #just changed 2*1 to 2*i - rerun for better feature. \n",
    "#     yules_i_feature.append(np.abs(yules_i))\n",
    "    \n",
    "# yules_i_feature = np.vstack(yules_i_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misspellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "def misspelled_words(text):\n",
    "    #Library for spell checking\n",
    "    spell = SpellChecker()\n",
    "    text = remove_symbols(text)\n",
    "    #Regex for finding digits\n",
    "    _digits = re.compile('\\d')\n",
    "\n",
    "    #List of misspelled words\n",
    "    misspelled = spell.unknown(text.split())\n",
    "    #Remove words, that start with capital letter (Likely names)\n",
    "    no_names = [x for x in misspelled if x.title() not in text]\n",
    "    #Remove words that contain digits (7th)\n",
    "    no_digits = [x for x in no_names if not bool(_digits.search(x))]\n",
    "    \n",
    "    #Find corrections for misspelled words - if word is more than a single character.\n",
    "    corrections = [spell.correction(x) for x in no_digits if len(x)>1]\n",
    "    #Remove corrections, if they have no correction (likely misclassified spelling mistake)\n",
    "    remove_no_correction = [x for x in corrections if x not in misspelled]\n",
    "    return remove_no_correction\n",
    "\n",
    "misspelled_words(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspellings_feature = []\n",
    "\n",
    "# for i in tqdm(range(len(text_pairs))):\n",
    "#     num_of_misspellings = len(misspelled_words(corpus[2*1])) - len(misspelled_words(corpus[2*i+1]))\n",
    "#     misspellings_feature.append(np.abs(num_of_misspellings))\n",
    "    \n",
    "# misspellings_feature = np.vstack(misspellings_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_character_ngram(n, corpus):\n",
    "    #Will return matrix where each row is a pair of texts\n",
    "    vectorizer = TfidfVectorizer(max_features=3000, analyzer='char',ngram_range=(n, n))\n",
    "    char_bigram = vectorizer.fit_transform(corpus).toarray()\n",
    "    return np.array([np.concatenate((char_bigram[x], char_bigram[x+1])) for x in range(0,char_bigram.shape[0],2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stolen from https://stackoverflow.com/questions/31847682/how-to-compute-skipgrams-in-python\n",
    "from itertools import chain, combinations\n",
    "import copy\n",
    "from nltk.util import ngrams\n",
    "\n",
    "corpus_tokens = [nltk.word_tokenize(x) for x in corpus]\n",
    "\n",
    "def pad_sequence(sequence, n, pad_left=False, pad_right=False, pad_symbol=None):\n",
    "    if pad_left:\n",
    "        sequence = chain((pad_symbol,) * (n-1), sequence)\n",
    "    if pad_right:\n",
    "        sequence = chain(sequence, (pad_symbol,) * (n-1))\n",
    "    return sequence\n",
    "\n",
    "def skipgrams(sequence, n, k, pad_left=False, pad_right=False, pad_symbol=None):\n",
    "    sequence_length = len(sequence)\n",
    "    sequence = iter(sequence)\n",
    "    sequence = pad_sequence(sequence, n, pad_left, pad_right, pad_symbol)\n",
    "\n",
    "    if sequence_length + pad_left + pad_right < k:\n",
    "        raise Exception(\"The length of sentence + padding(s) < skip\")\n",
    "\n",
    "    if n < k:\n",
    "        raise Exception(\"Degree of Ngrams (n) needs to be bigger than skip (k)\")    \n",
    "\n",
    "    history = []\n",
    "    nk = n+k\n",
    "\n",
    "    # Return point for recursion.\n",
    "    if nk < 1: \n",
    "        return\n",
    "    # If n+k longer than sequence, reduce k by 1 and recur\n",
    "    elif nk > sequence_length: \n",
    "        for ng in skipgrams(list(sequence), n, k-1):\n",
    "            yield ng\n",
    "\n",
    "    while nk > 1: # Collects the first instance of n+k length history\n",
    "        history.append(next(sequence))\n",
    "        nk -= 1\n",
    "\n",
    "    # Iterative drop first item in history and picks up the next\n",
    "    # while yielding skipgrams for each iteration.\n",
    "    for item in sequence:\n",
    "        history.append(item)\n",
    "        current_token = history.pop(0)      \n",
    "        # Iterates through the rest of the history and \n",
    "        # pick out all combinations the n-1grams\n",
    "        for idx in list(combinations(range(len(history)), n-1)):\n",
    "            ng = [current_token]\n",
    "            for _id in idx:\n",
    "                ng.append(history[_id])\n",
    "            yield tuple(ng)\n",
    "\n",
    "    # Recursively yield the skigrams for the rest of seqeunce where\n",
    "    # len(sequence) < n+k\n",
    "    for ng in list(skipgrams(history, n, k-1)):\n",
    "        yield ng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_features(feature_dict):\n",
    "    '''Save the updated feature dictionary. Takes dictionary as input and saves as binary file\n",
    "    \n",
    "    example: \n",
    "    >>> my_featues = {'freqdist': [1,6,3,5]}\n",
    "    >>> save_features(my_features)'''\n",
    "    \n",
    "    with open('data/features.dat', 'wb') as file:\n",
    "        pickle.dump(feature_dict, file)\n",
    "    print(\"Features saved! :-)\")\n",
    "\n",
    "def load_features():\n",
    "    '''Load feature dictionary. Returns the saved feature as a dictionary.\n",
    "    Will then print all the available features.\n",
    "    \n",
    "    example: \n",
    "    >>> my_features = load_features()'''\n",
    "    \n",
    "    with open('data/features.dat', 'rb') as file:\n",
    "        feats = pickle.load(file)\n",
    "    print(\"Features available:\")\n",
    "    for i in feats.keys():\n",
    "        print(i)\n",
    "    \n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat_matrix = function words, profanity words, avg sentence length, avg word length, lix, yules i, number of misspellings\n",
    "feat_matrix = np.load(\"feature_matrix.dat\", allow_pickle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_scalars = preprocessing.normalize(feat_matrix[:,2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.hstack((feat_matrix[:,:2],normalized_scalars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_lix = feat_matrix[:,[0,1,2,3,5]]\n",
    "without_avg_word = feat_matrix[:,[0,1,2,4,5]]\n",
    "without_fw = feat_matrix[:,[1,2,3,4,5]]\n",
    "without_profanity = feat_matrix[:,[0,2,3,4,5]]\n",
    "without_avg_sent = feat_matrix[:,[0,1,3,4,5]]\n",
    "without_yules = feat_matrix[:,[0,1,2,3,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feat_matrix, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = svm_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf.dual_coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_clf = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = random_forest_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, prediction) #random_state decreased the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on feature combinations: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When train/test split is 80/20\n",
    "\n",
    "Features = function words, profanity words, avg sentence length, avg word length, lix, yules i, number of misspellings difference\n",
    "\n",
    "\n",
    "**All:** \n",
    "    SVM acc = **0.617**0886075949367,\n",
    "    RF acc = **0.724**6835443037974 \n",
    "\n",
    "**Without number of misspellings difference:** \n",
    "    SVM acc = **0.648**7341772151899,\n",
    "    RF acc = **0.721**5189873417721\n",
    "    \n",
    "**Without lix:** \n",
    "    SVM acc = **0.655**0632911392406,\n",
    "    RF acc = **0.727**8481012658228\n",
    "    \n",
    "**Without yules I:** \n",
    "    SVM acc = **0.645**5696202531646,\n",
    "    RF acc = **0.712**0253164556962\n",
    "\n",
    "**Without average sentence length:**\n",
    "    SVM acc = **0.563**2911392405063,\n",
    "    RF acc = **0.718**3544303797469\n",
    "\n",
    "**Without average word length:**\n",
    "    SVM acc = **0.642**4050632911392,\n",
    "    RF acc = **0.674**0506329113924\n",
    "    \n",
    "**Without function words:** \n",
    "    SVM acc = **0.645**5696202531646,\n",
    "    RF acc = **0.648**7341772151899\n",
    "    \n",
    "**Without profanity words:** \n",
    "    SVM acc = **0.645**5696202531646,\n",
    "    RF acc = **0.699**3670886075949 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[1426]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
