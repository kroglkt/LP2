{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_emot():\n",
    "    e = [\"(o_o)\",\":-)\",\":P\",\":D\",\"x)\",\"ᓚᘏᗢ\",\"╯°□°）╯︵ ┻━┻\",\":)\",\n",
    "         \"*<:-)\",\"^_^\",\"(⌐■_■)\",\"¯\\_(ツ)_/¯\", \"(T_T)\",\":o\",\"OwO\",\n",
    "        \"( ͡❛ ͜ʖ ͡❛)\",\"(̶◉͛‿◉̶)\",\"( ≖.≖)\",\"(ㆆ_ㆆ)\",\"ʕ•́ᴥ•̀ʔっ\",\"( ◡́.◡̀)\",\"(^◡^ )\"]\n",
    "    return random.choice(e)\n",
    "\n",
    "def load_files():\n",
    "    text_pairs = [] #Would be nice to have as np.array\n",
    "    labels = []\n",
    "    fandom = []\n",
    "    \n",
    "    pair_id = []\n",
    "    true_id = []\n",
    "    \n",
    "    #Load truth JSON\n",
    "    for line in open('data/modified/train_truth.jsonl'):\n",
    "        d = json.loads(line.strip())\n",
    "        labels.append(int(d['same']))\n",
    "        true_id.append(d['id'])\n",
    "\n",
    "    #Load actual fanfic.\n",
    "    print(\"loading fanfic...\",rand_emot())\n",
    "    for line in tqdm(open('data/modified/train_pair.jsonl')):\n",
    "        d = json.loads(line.strip())\n",
    "        text_pairs.append(d['pair'])\n",
    "        fandom.append(d['fandoms'])\n",
    "        pair_id.append(d['id'])\n",
    "\n",
    "    print(\"done loading\",rand_emot())\n",
    "    \n",
    "    return text_pairs, labels, fandom, pair_id, true_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "193it [00:00, 1911.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading fanfic... ¯\\_(ツ)_/¯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1578it [00:00, 1886.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading ^_^\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_pairs, labels, fandom, pair_id, true_id = load_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word frequency and word frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_distribution(text_pair): #expect untokenized input\n",
    "    \n",
    "    pair = []\n",
    "    \n",
    "    for text in text_pair: \n",
    "        tokens = nltk.word_tokenize(text) #tokenize\n",
    "        \n",
    "        freq_dist = nltk.FreqDist(tokens) #compute frequency distribution\n",
    "        pair.append(freq_dist)\n",
    "        \n",
    "    return pair #return frequency distribution of each fanfic in the input pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq(text_pair): #expects tokenized pairs\n",
    "    fdist0 = nltk.FreqDist(text_pair[0])\n",
    "    fdist1 = nltk.FreqDist(text_pair[1])\n",
    "    \n",
    "    return [fdist0, fdist1]\n",
    "\n",
    "def word_freq_single(text):\n",
    "    fdist = nltk.FreqDist(text)\n",
    "    return fdist\n",
    "\n",
    "def tokenize(text_pair):\n",
    "    return [nltk.word_tokenize(text_pair[0]),nltk.word_tokenize(text_pair[1])]\n",
    "\n",
    "def vector_freq_dist(freq_dists): #I don't think this works...\n",
    "    return [list(freq_dists[0].values()), list(freq_dists[1].values())]\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(text_pairs):\n",
    "    '''input all text pairs to create a corpus'''\n",
    "    corpus = [x[i] for x in text_pairs for i in range(len(x))]\n",
    "    return corpus\n",
    "\n",
    "def fit_tfidf(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    print(\"training vectorizer...\",rand_emot())\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    print(\"vectorizer fit!\", rand_emot())\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(X[0].T.todense(), index=vectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "    df = df.sort_values('TF-IDF', ascending=False)\n",
    "    \n",
    "    return X, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = create_corpus(text_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training vectorizer... (ㆆ_ㆆ)\n",
      "vectorizer fit! :)\n"
     ]
    }
   ],
   "source": [
    "#tf-idf on the raw text. Likely not useful, as you can see, it is sesnitive to the fandom.\n",
    "raw_tfidf, tfidf_df = fit_tfidf(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TF-IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.389299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.295670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kuroko</th>\n",
       "      <td>0.283424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>judgement</th>\n",
       "      <td>0.247891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>0.184794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.182388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>0.182330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>0.172474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>she</th>\n",
       "      <td>0.169618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>her</th>\n",
       "      <td>0.146488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             TF-IDF\n",
       "the        0.389299\n",
       "to         0.295670\n",
       "kuroko     0.283424\n",
       "judgement  0.247891\n",
       "was        0.184794\n",
       "and        0.182388\n",
       "it         0.182330\n",
       "that       0.172474\n",
       "she        0.169618\n",
       "her        0.146488"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempting to perform tf-idf on only symbols.\n",
    "def isolate_symbols(corpus):\n",
    "    #Add \\d to omit digits too.\n",
    "    sym_corpus = []\n",
    "    for text in corpus:\n",
    "        sym_corpus.append(' '.join(re.findall(\"[^a-zA-Z\\s]+\", text)))\n",
    "    return sym_corpus\n",
    "\n",
    "symbols = isolate_symbols(corpus)\n",
    "\n",
    "#Okay, tf-idf doesn't work with symbols. I'll convert them to made-up words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS-tagging and Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## POS Tagging and ngrams\n",
    "tokens = nltk.word_tokenize(corpus[0])\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "pos_bigrams = nltk.bigrams(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in corpus[:10]:\n",
    "    print(compute_lix(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIX calculation - LIX = readability index, a measure for the readability of a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lix(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    splt = text.split()\n",
    "    o = len(splt)+1\n",
    "    p = len([x for x in tokens if x=='.'])+1\n",
    "    l = len([x for x in tokens if len(x)>6])+1\n",
    "    \n",
    "    return (o/p)+((l*100)/o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence and word length - compute sentence and word length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5d1cd516962a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msentence_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_lengths\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mget_sent_word_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "def remove_symbols(text):\n",
    "    sentences = re.split('[\\.+|!|?]', text)\n",
    "    sentences = [re.sub(r\"[^\\w]+\", ' ', x) for x in sentences if len(x.strip()) != 0]\n",
    "    return ' '.join(sentences)\n",
    "\n",
    "def get_sent_word_length(text):\n",
    "    #Function, which removes symbols and count words in sentence\n",
    "    #Output: length of each sentence & length of each word\n",
    "    sentences = re.split('[\\.+|!|?]', text)\n",
    "    sentences = [re.sub(r\"[^\\w]+\", ' ', x) for x in sentences if len(x.strip()) != 0]\n",
    "    word_sentences = [nltk.word_tokenize(x) for x in sentences]\n",
    "    sentence_lengths = np.array([len(x) for x in word_sentences])\n",
    "    word_lengths = np.array([len(s) for x in word_sentences for s in x])\n",
    "    return sentence_lengths, word_lengths\n",
    "\n",
    "get_sent_word_length(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolating function words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'data/modified/train_pair.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/function_words_clean.txt', \"r\") as fw:\n",
    "    func_words = fw.read().split()\n",
    "\n",
    "def isolate_fw(data, f_words): #data must be json file - input must be path to file \n",
    "    fw_in_data = []\n",
    "\n",
    "    for line in tqdm(open(data)):\n",
    "        function_words = []\n",
    "        d = json.loads(line.strip()) #load the json file\n",
    "        text = d.get(\"pair\") #get the actual fanfic\n",
    "        words = text[0].split() #split fanfic into words in list\n",
    "        for word in words: \n",
    "            if word in f_words: #if the word is a function word\n",
    "                function_words.append(word)\n",
    "                \n",
    "        stringed_function_words = \" \".join(function_words)\n",
    "        \n",
    "        #append all function words as one long string in a list\n",
    "        fw_in_data.append([stringed_function_words]) #fw_in_data is a list with lists\n",
    "        #each list contains a string of all function words for each pair\n",
    "        #should it be a string for each pair?\n",
    "        \n",
    "    return fw_in_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1578it [00:30, 51.67it/s]\n"
     ]
    }
   ],
   "source": [
    "test = isolate_fw(data, func_words) #why is this so fast?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to simplifying isolation of function words - make it take text_pairs as input so we only load the files once\n",
    "# and use the same kind of input in our various functions\n",
    "\n",
    "#this made it hella slow - fix it!\n",
    "\n",
    "def isolate_fw_2(data, f_words): #data must be the text_pairs from load_files()\n",
    "    fw_in_data = []\n",
    "    \n",
    "    fw_text_pairs = []\n",
    "    for pair in tqdm(data):\n",
    "        fw_text_pairs = []\n",
    "        for text in pair: \n",
    "            function_words = []\n",
    "            \n",
    "            words = text.split() #split fanfic into words in list\n",
    "            \n",
    "            for word in words: \n",
    "                if word in f_words: #if the word is a function word\n",
    "                    function_words.append(word)\n",
    "                \n",
    "            stringed_function_words = \" \".join(function_words) #for each fanfic in a pair, makes FW a long string. \n",
    "            fw_text_pairs.append(stringed_function_words) \n",
    "            \n",
    "        #append text pairs with only their function words\n",
    "        fw_in_data.append(fw_text_pairs) \n",
    "        \n",
    "    return fw_in_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 1578/1578 [00:54<00:00, 29.04it/s]\n"
     ]
    }
   ],
   "source": [
    "fw_inData_2 = isolate_fw_2(text_pairs, func_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FreqDist({'the': 134, 'to': 118, 'a': 91, 'was': 72, 'and': 64, 'of': 58, 'in': 53, 'her': 45, 'that': 43, 'for': 38, ...}),\n",
       " FreqDist({'the': 111, 'and': 102, 'to': 98, 'a': 61, 'her': 56, 'his': 47, 'of': 45, 'she': 42, 'in': 41, 'he': 37, ...})]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_distribution(fw_inData_2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolating profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'data/modified/train_pair.jsonl'\n",
    "with open('data/profanity_words_clean.txt', \"r\") as pr:\n",
    "    prof_words = pr.read().split()\n",
    "del prof_words[:4]\n",
    "del prof_words[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'data/modified/train_pair.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_profanity(data, prof_words):\n",
    "    profanity = []\n",
    "    \n",
    "    for pair in tqdm(data):\n",
    "        profanity_pairs = []\n",
    "        for text in pair:\n",
    "            resultwords = []\n",
    "\n",
    "            #d = json.loads(line.strip())\n",
    "            #text = d.get(\"pair\") \n",
    "            words = text.split() \n",
    "\n",
    "            resultwords  = [word for word in words if word.lower() in prof_words]\n",
    "\n",
    "            result = \" \".join(resultwords)\n",
    "            profanity_pairs.append(result)\n",
    "        \n",
    "        profanity.append(profanity_pairs) \n",
    "\n",
    "        \n",
    "    return profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 1578/1578 [01:59<00:00, 13.22it/s]\n"
     ]
    }
   ],
   "source": [
    "profanity_inData = isolate_profanity(text_pairs, prof_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FreqDist({'balls': 5, 'bastard': 1, 'hell': 1}),\n",
       " FreqDist({'bloody': 2, 'shit': 2, 'hell': 2, 'fucking': 2})]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_distribution(profanity_inData[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yule's K computations - different implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Our own implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_no_symbols(text):\n",
    "    return nltk.word_tokenize(re.sub(r'[^\\w]', ' ', text))\n",
    "\n",
    "def get_fdist_yule(text):\n",
    "    text = tokenize_no_symbols(text)\n",
    "    fdist = word_freq_single(text)\n",
    "    return fdist\n",
    "        \n",
    "def get_num_unique_words(text):\n",
    "    text = tokenize_no_symbols(text.lower())\n",
    "    return len(set(text))\n",
    "        \n",
    "def yules_k(text):\n",
    "    C = 10000\n",
    "    splt = text.split()\n",
    "    N = len(splt)\n",
    "    Vn = get_num_unique_words(text)\n",
    "    fdist = get_fdist_yule(text)\n",
    "    max_word = fdist.most_common()[0][1]\n",
    "    \n",
    "    var = 0\n",
    "    \n",
    "    for m in range(max_word):\n",
    "        Vmn = len([x for x in fdist if fdist[x]==m])\n",
    "        mNs = np.power(m/N,2)\n",
    "        \n",
    "        var += Vmn*mNs\n",
    "    \n",
    "    return C*(-1/N+var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-9fc2d49c2a97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myules_k\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_lix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
     ]
    }
   ],
   "source": [
    "for i in corpus[:10]:\n",
    "    k,lix = yules_k(i), compute_lix(i)\n",
    "    print(k, lix, k-lix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.62083541967105\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-e660697cc080>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0myules_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myules_k\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myules_k\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
     ]
    }
   ],
   "source": [
    "for i in corpus[:10]:\n",
    "    yules_k = yules_k(i)\n",
    "    print(yules_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Implementation below from:\n",
    "https://gist.github.com/magnusnissel/d9521cb78b9ae0b2c7d6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "\n",
    "def tokenize(s):\n",
    "    tokens = re.split(r\"[^0-9A-Za-z\\-'_]+\", s)\n",
    "    return tokens\n",
    "\n",
    "def get_yules(s):\n",
    "    \"\"\" \n",
    "    Returns a tuple with Yule's K and Yule's I.\n",
    "    (cf. Oakes, M.P. 1998. Statistics for Corpus Linguistics.\n",
    "    International Journal of Applied Linguistics, Vol 10 Issue 2)\n",
    "    In production this needs exception handling.\n",
    "    \"\"\"\n",
    "    tokens = tokenize(s)\n",
    "    token_counter = collections.Counter(tok.upper() for tok in tokens)\n",
    "    m1 = sum(token_counter.values())\n",
    "    m2 = sum([freq ** 2 for freq in token_counter.values()])\n",
    "    i = (m1*m1) / (m2-m1)\n",
    "    k = 1/i * 10000\n",
    "    return (k, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76.24848330128657, 131.15014970837154)\n",
      "(66.47285249322603, 150.4373533694685)\n",
      "(93.3293041366282, 107.1474826959026)\n",
      "(106.93616216764885, 93.51373564653021)\n",
      "(109.14087031159232, 91.62470458088198)\n",
      "(91.70043097696845, 109.05074156643394)\n",
      "(83.6476796371318, 119.54904240476895)\n",
      "(94.47986224640348, 105.8426606711174)\n",
      "(83.97021309490067, 119.08984902418068)\n",
      "(88.22340444450815, 113.34860701606596)\n"
     ]
    }
   ],
   "source": [
    "for text in corpus[:10]: \n",
    "    k_i = get_yules(text)\n",
    "    print(k_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Implementation below from: https://swizec.com/blog/measuring-vocabulary-richness-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from itertools import groupby\n",
    "\n",
    "def words(entry):\n",
    "    return filter(lambda w: len(w) > 0,\n",
    "                  [w.strip(\"0123456789!:,.?(){}[]\") for w in entry.split()])\n",
    "\n",
    "def yule(entry):\n",
    "    # yule's I measure (the inverse of yule's K measure)\n",
    "    # higher number is higher diversity - richer vocabulary\n",
    "    d = {}\n",
    "    stemmer = PorterStemmer()\n",
    "    for w in words(entry):\n",
    "        w = stemmer.stem(w).lower()\n",
    "        try:\n",
    "            d[w] += 1\n",
    "        except KeyError:\n",
    "            d[w] = 1\n",
    "\n",
    "    M1 = float(len(d))\n",
    "    M2 = sum([len(list(g))*(freq**2) for freq,g in groupby(sorted(d.values()))])\n",
    "\n",
    "    try:\n",
    "        return (M1*M1)/(M2-M1)\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.645130247537017\n",
      "17.728321988754068\n",
      "6.742396307162275\n",
      "8.927175336111278\n",
      "7.337997328744253\n",
      "7.3147799405225005\n",
      "8.430640063377464\n",
      "11.789875705444068\n",
      "9.429946152509988\n",
      "10.677045985641614\n"
     ]
    }
   ],
   "source": [
    "for text in corpus[:10]: \n",
    "    yules_i = yule(text) #yules_i = inverse of yules K  - why do you want the inverse instead? \n",
    "    print(yules_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<filter object at 0x000001F5C7F2C0A0>\n",
      "<filter object at 0x000001F5C7F2C5B0>\n"
     ]
    }
   ],
   "source": [
    "for text in corpus[:2]: \n",
    "    tokenized_words = words(text)  \n",
    "    print(tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misspellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'remove_symbols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-2b5c96467dfe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mremove_no_correction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mmisspelled_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-2b5c96467dfe>\u001b[0m in \u001b[0;36mmisspelled_words\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#Library for spell checking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mspell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSpellChecker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_symbols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m#Regex for finding digits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0m_digits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\d'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'remove_symbols' is not defined"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "def misspelled_words(text):\n",
    "    #Library for spell checking\n",
    "    spell = SpellChecker()\n",
    "    text = remove_symbols(text)\n",
    "    #Regex for finding digits\n",
    "    _digits = re.compile('\\d')\n",
    "\n",
    "    #List of misspelled words\n",
    "    misspelled = spell.unknown(text.split())\n",
    "    #Remove words, that start with capital letter (Likely names)\n",
    "    no_names = [x for x in misspelled if x.title() not in text]\n",
    "    #Remove words that contain digits (7th)\n",
    "    no_digits = [x for x in no_names if not bool(_digits.search(x))]\n",
    "    \n",
    "    #Find corrections for misspelled words - if word is more than a single character.\n",
    "    corrections = [spell.correction(x) for x in no_digits if len(x)>1]\n",
    "    #Remove corrections, if they have no correction (likely misclassified spelling mistake)\n",
    "    remove_no_correction = [x for x in corrections if x not in misspelled]\n",
    "    return remove_no_correction\n",
    "\n",
    "misspelled_words(corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
